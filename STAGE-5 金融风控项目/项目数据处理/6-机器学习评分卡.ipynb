{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 机器学习评分卡学习目标：\n",
    "- 掌握KS值计算方法\n",
    "- 知道评分映射方法\n",
    "- 知道XGBoost和LightGBM基本原理\n",
    "- 掌握使用lightGBM进行特征筛选的方法\n",
    "- 应用Toad构建评分卡模型\n",
    "\n",
    "\n",
    "# 1、 什么是Gradient Boosting算法？\n",
    "`基本原理`：训练一个模型m1，产生一个错误e1，针对e1训练一个模型m2，产生错误e2，针对e2训练第三个模型m3，产生错误e3，以此类推，最终的预测结果是：m1 + m2 + m3……\n",
    "\n",
    "`核心思想`：GBDT是boosting的一种方法，每次简历单个分类器，是在之前建立的模型的损失函数的梯度下降方法，损失函数越大说明模型跟容易出错，如果我们能让损失函数持续下降，则说明我们的模型在持续不断的改进，而最好的方式就是让损失函数在其梯度的方向上下降\n",
    "GBDT的核心就在于每一棵树学的都是之前所有树的结论和残差\n",
    "- 残差就是真实值与预测值之间的差值\n",
    "- 为了得到残差，GBDT中树全部都是回归树，没有分类树\n",
    "\n",
    "`GBDT和随机森林的异同点`：\n",
    "\n",
    "**相同点**：\n",
    "   - 两者都是基于决策树的集成学习方法。\n",
    "   - 都适用于回归和分类任务。\n",
    "   - 都可以通过调整树的数量、深度等超参数来控制模型复杂度。\n",
    "\n",
    "**不同点**：\n",
    "   - **核心思想**：GBDT 是 Boosting 方法，强调逐步优化；随机森林是 Bagging 方法，强调多样性。\n",
    "   - **训练方式**：GBDT 是串行训练，随机森林是并行训练。\n",
    "   - **对异常值的敏感性**：GBDT 更敏感，随机森林更鲁棒。\n",
    "   - **适用场景**：GBDT 更适合低噪声数据，随机森林更适合高噪声数据。\n",
    "    \n",
    "\n",
    "| **对比维度**            | **GBDT**                                                                                      | **随机森林**                                                                                  |\n",
    "|-------------------------|----------------------------------------------------------------------------------------------|----------------------------------------------------------------------------------------------|\n",
    "| **基本思想**            | 基于提升（Boosting）思想，通过迭代训练多个弱学习器，每一轮都关注前一轮的错误进行修正。          | 基于装袋（Bagging）思想，通过并行训练多个独立的决策树，最终通过投票或平均的方式集成结果。      |\n",
    "| **模型构建方式**        | 串行构建，每棵树依赖于前一棵树的结果，逐步减少残差（误差）。                                   | 并行构建，每棵树独立生成，互不依赖。                                                         |\n",
    "| **树的数量影响**        | 树的数量过多可能导致过拟合，因为每棵树都在“修正”前一棵树的错误。                               | 树的数量越多通常越稳定，但收益递减，且不会显著增加过拟合风险。                                |\n",
    "| **样本采样方式**        | 使用全部样本训练每棵树，但通过梯度下降的方式调整每一轮的权重分布。                              | 对样本进行自助采样（Bootstrap Sampling），每棵树使用不同的子集进行训练。                     |\n",
    "| **特征选择方式**        | 每棵树使用所有特征，通过分裂时的最优特征选择进行建模。                                         | 每棵树在分裂节点时随机选择部分特征，进一步增加了多样性。                                      |\n",
    "| **对异常值的敏感性**    | 对异常值较为敏感，因为每一轮都会基于残差优化，异常值可能会被过度关注。                           | 对异常值不太敏感，因为每棵树是独立的，异常值的影响会被平均化。                                 |\n",
    "| **训练速度**            | 训练速度较慢，因为每棵树依赖于前一棵树，无法并行计算。                                          | 训练速度较快，因为每棵树可以独立并行生成。                                                    |\n",
    "| **预测速度**            | 预测速度较慢，因为需要累加多棵树的预测结果。                                                   | 预测速度较快，因为可以直接通过投票或平均得出结果。                                            |\n",
    "| **适用场景**            | 更适合处理低噪声数据，擅长捕捉复杂模式，适用于回归和分类问题。                                  | 更适合处理高噪声数据，擅长处理高维稀疏数据，适用于回归和分类问题。                             |\n",
    "| **防止过拟合的方式**    | 通过限制树的深度、学习率（Shrinkage）、正则化等手段控制模型复杂度。                             | 通过增加树的数量、限制树的深度、随机采样等方式增加模型多样性，从而降低过拟合风险。              |\n",
    "| **模型解释性**          | 较难解释，因为每棵树都在修正前一棵树的误差，整体模型是一个复杂的加权组合。                      | 较易解释，因为每棵树是独立的，最终结果可以通过投票或平均直观理解。                              |\n",
    "| **典型实现库**          | XGBoost、LightGBM、CatBoost 等。                                                              | Scikit-learn 中的 `RandomForestClassifier` 和 `RandomForestRegressor`。                       |\n",
    "\n",
    "\n",
    "`GBDT的优缺点`：\n",
    "\n",
    "| **优点**                                | **缺点**                                    |\n",
    "|-----------------------------------------|---------------------------------------------|\n",
    "| 高预测精度                              | 训练速度较慢                                |\n",
    "| 处理非线性关系能力强                    | 调参复杂                                    |\n",
    "| 自动捕捉特征交互                        | 对异常值敏感                                |\n",
    "| 支持多种损失函数                        | 数据预处理要求较高                          |\n",
    "| 不容易过拟合（适当调参后）              | 容易过拟合（如果调参不当）                  |\n",
    "| 易于扩展（如 XGBoost、LightGBM 等）     | 模型解释性较差                              |\n",
    "|                                         | 内存占用较大                                |\n",
    "\n",
    "**改进方向**\n",
    "- 使用改进版框架（如 XGBoost、LightGBM、CatBoost），它们在训练速度、内存占用、特征处理等方面进行了优化。\n",
    "- 结合其他模型（如深度学习），在特定场景下实现混合建模。\n",
    "\n",
    "\n",
    "\n",
    "# 2、什么是XGBoost？\n",
    "`基本原理`：XGBoost（Extreme Gradient Boosting）与GBDT原理相同，它是经过优化的分布式梯度提升库，更高效、灵活、可移植，并且XGBoost是大规模并行Boosting tree 的工具，比GBDT更高效\n",
    "\n",
    "`和XGBoost区别`： \n",
    "- XGBoost生成的CART树考虑了树的复杂度，而GBDT是在剪枝步骤中才考虑了树的复杂度\n",
    "- XGBoost是拟合上一轮的损失函数的二阶导展开，GBDT是拟合上一轮损失函数一阶展开，因此XGBoost精确性更高且相同训练效果下迭代次数更少\n",
    "- XGBoost与GBDT都是在主次迭代提高模型性能，但是XGBoost是选取最佳切分点是可以开启多线程进行，提高效率\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "2d6de75f4b3e6b08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T09:24:29.306488Z",
     "start_time": "2025-03-15T09:24:28.931860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 导入pandas包\n",
    "import pandas as pd\n",
    "# 导入numpy包\n",
    "import numpy as np\n",
    "# 导入random包\n",
    "import random\n",
    "# 导入数学包\n",
    "import math\n",
    "# 导入sklearn度量工具\n",
    "from sklearn import metrics\n",
    "# 导入sklearn度量工具\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "# 导入model_selection库包含：数据分割、交叉验证、超参数搜索和模型评估\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 导入sklearn线性回归库\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# 1、数据读取\n",
    "data = pd.read_csv('./file/Bcard.txt')\n",
    "'''\n",
    "    数据量：95806行\n",
    "    字段数量：13\n",
    "    存在空字段：无\n",
    "    数据类型：float64、object(2)\n",
    "    占用内存：9.5+ MB\n",
    "'''\n",
    "data.info()\n",
    "data.head()"
   ],
   "id": "5444d48ba1a386d",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:09:56.833520Z",
     "start_time": "2025-03-15T07:09:56.022003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1、查看月份分布，用最后一个月作为时间外样本\n",
    "'''\n",
    "“时间外样本”（Out-of-Time Sample）是机器学习和数据分析中一个重要的概念，特别是在处理时间序列数据或涉及时间因素的建模任务中。时间外样本是指那些在模型训练过程中未被使用的、时间上更靠后的数据，用于评估模型在新数据上的泛化能力。\n",
    "'''\n",
    "print('时间外样本：', data['obs_mth'].unique())\n",
    "\n",
    "# 2、划分数据集\n",
    "train = data[data['obs_mth'] != '2018-11-30'].reset_index().copy()\n",
    "val = data[data['obs_mth'] == '2018-11-30'].reset_index().copy()\n",
    "\n",
    "# 3、取出建模用到的特征\n",
    "feature_lst = ['person_info', 'finance_info', 'credit_info', 'act_info', 'td_score', 'jxl_score', 'mj_score',\n",
    "               'rh_score']\n",
    "\n",
    "# 4、模型训练\n",
    "x = train[feature_lst]\n",
    "y = train['bad_ind']\n",
    "\n",
    "first_row = x.iloc[0]\n",
    "print(\"特征值:\", first_row[:-1], \"目标值:\", y.iloc[0])\n",
    "\n",
    "val_x = val[feature_lst]\n",
    "val_y = val['bad_ind']\n",
    "lr_model = LogisticRegression(C=0.1)\n",
    "lr_model.fit(x, y)\n",
    "\n",
    "'''\n",
    "TPR：表示在所有实际为正的样本中，模型正确预测为正的比例\n",
    "FPR：表示在所有实际为负的样本中，模型错误预测为正的比例\n",
    "TPR 衡量模型对正样本的识别能力，而 FPR 衡量模型对负样本的误判能力\n",
    "'''\n",
    "# 5、计算测试集和训练集TPR和FPR\n",
    "# 训练集取出预测值\n",
    "y_predict = lr_model.predict_proba(x)[:, 1]\n",
    "# 计算TPR和FPR\n",
    "fpr_lr_train, tpr_lr_train, _ = roc_curve(y, y_predict)\n",
    "# 计算KS\n",
    "train_ks = abs(fpr_lr_train - tpr_lr_train).max()\n",
    "print('train_ks : ', train_ks)\n",
    "\n",
    "# 验证集计算预测值\n",
    "y_predict = lr_model.predict_proba(val_x)[:, 1]\n",
    "# 计算预测值\n",
    "fpr_lr, tpr_lr, _ = roc_curve(val_y, y_predict)\n",
    "# 计算KS值\n",
    "val_ks = abs(fpr_lr - tpr_lr).max()\n",
    "print('val_ks : ', val_ks)\n",
    "\n",
    "# 6、绘制图像\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(fpr_lr_train, tpr_lr_train, label='train LR')  #绘制训练集ROC\n",
    "plt.plot(fpr_lr, tpr_lr, label='evl LR')  #绘制验证集ROC\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ],
   "id": "f09255cdf88d8686",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "f2752a85ae8ef7df",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:10:01.332486Z",
     "start_time": "2025-03-15T07:09:59.391174Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 导入lightGBM包\n",
    "import lightgbm as lgb\n",
    "# 导入sklearn中模型选择工具\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def lgb_test(train_x, train_y, test_x, test_y):\n",
    "    '''\n",
    "    lightGBM进行特征筛选\n",
    "    :param train_x: \n",
    "    :param train_y: \n",
    "    :param test_x: \n",
    "    :param test_y: \n",
    "    :return: \n",
    "    '''\n",
    "    clf = lgb.LGBMClassifier(boosting_type='gbdt',\n",
    "                             objective='binary',\n",
    "                             metric='auc',\n",
    "                             learning_rate=0.1,\n",
    "                             n_estimators=24,\n",
    "                             max_depth=5,\n",
    "                             num_leaves=20,\n",
    "                             max_bin=45,\n",
    "                             min_data_in_leaf=6,\n",
    "                             bagging_fraction=0.6,\n",
    "                             bagging_freq=0,\n",
    "                             feature_fraction=0.8, )\n",
    "    clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (test_x, test_y)], eval_metric='auc')\n",
    "    return clf, clf.best_score_['valid_1']['auc'],\n",
    "\n",
    "\n",
    "# 1、数据集划分\n",
    "train_x, test_x, train_y, test_y = train_test_split(x, y, random_state=0, test_size=0.2)\n",
    "\n",
    "lgb_model, lgb_auc = lgb_test(train_x, train_y, test_x, test_y)\n",
    "dict_arr = {\n",
    "    'name': lgb_model.booster_.feature_name(),\n",
    "    'importance': lgb_model.feature_importances_\n",
    "}\n",
    "feature_importance = pd.DataFrame(dict_arr).sort_values(by=['importance'], ascending=False)"
   ],
   "id": "f445229862af01d4",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:17:01.600666Z",
     "start_time": "2025-03-15T11:17:01.162140Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 1、模型调优，去掉几个特征，重新建模 确定新的特征\n",
    "feature_lst = ['person_info', 'finance_info', 'credit_info', 'act_info']\n",
    "x = train[feature_lst]\n",
    "y = train['bad_ind']\n",
    "\n",
    "val_x = val[feature_lst]\n",
    "val_y = val['bad_ind']\n",
    "\n",
    "# 2、模型训练\n",
    "lr_model = LogisticRegression(C=0.1)\n",
    "lr_model.fit(x, y)\n",
    "\n",
    "# 3、计算FPR和TPR\n",
    "y_predict = lr_model.predict_proba(x)[:, 1]\n",
    "fpr_lr_train, tpr_lr_train, _ = roc_curve(y, y_predict)\n",
    "train_ks = abs(fpr_lr_train - tpr_lr_train).max()\n",
    "print('train_ks : ', train_ks)\n",
    "\n",
    "y_predict = lr_model.predict_proba(val_x)[:, 1]\n",
    "fpr_lr, tpr_lr, _ = roc_curve(val_y, y_predict)\n",
    "val_ks = abs(fpr_lr - tpr_lr).max()\n",
    "print('val_ks : ', val_ks)\n",
    "\n",
    "# 4、打印回归系数\n",
    "print('变量名单：', feature_lst)\n",
    "print('系数：', lr_model.coef_)\n",
    "print('截距：', lr_model.intercept_)\n",
    "\n",
    "# 5、绘图\n",
    "plt.plot(fpr_lr_train, tpr_lr_train, label='train LR')\n",
    "plt.plot(fpr_lr, tpr_lr, label='evl LR')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "# 6、生成报告"
   ],
   "id": "4c51f9cff186462d",
   "execution_count": 21,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:17:04.369317Z",
     "start_time": "2025-03-15T11:17:04.298613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1、准备数据\n",
    "bins = 20\n",
    "temp_ = pd.DataFrame()\n",
    "# 预测结果（坏人概率）\n",
    "temp_['bad_rate_predict'] = lr_model.predict_proba(val_x)[:, 1]\n",
    "# 真实结果\n",
    "temp_['real_bad'] = val_y\n",
    "# 按照预测坏人概率降序排列\n",
    "temp_ = temp_.sort_values('bad_rate_predict', ascending=False)\n",
    "#添加序号列，用于分组\n",
    "temp_['num'] = [i for i in range(temp_.shape[0])]\n",
    "#分成20组，为每组添加组号\n",
    "temp_['num'] = pd.cut(temp_.num, bins=bins, labels=[i for i in range(bins)])\n",
    "\n",
    "# 2、创建报告\n",
    "report = pd.DataFrame()\n",
    "# 计算每一组坏人数量\n",
    "report['BAD'] = temp_.groupby('num').real_bad.sum().astype(int)\n",
    "# 计算每一组好人数量\n",
    "report['GOOD'] = temp_.groupby('num').real_bad.count().astype(int) - report['BAD']\n",
    "# 累计求和坏人数量\n",
    "report['BAD_CNT'] = report['BAD'].cumsum()\n",
    "# 累计求和好人数量\n",
    "report['GOOD_CNT'] = report['GOOD'].cumsum()\n",
    "good_total = report.GOOD_CNT.max()\n",
    "bad_total = report.BAD_CNT.max()\n",
    "# 计算到当前组坏人比例（占所有坏人比例）\n",
    "report['BAD_PCTG'] = round(report.BAD_CNT / bad_total, 3)\n",
    "# 计算当前组坏人概率\n",
    "report['BADRATE'] = report.apply(lambda x: round(x.BAD / (x.BAD + x.GOOD), 3), axis=1)\n",
    "\n",
    "\n",
    "# 3、计算KS值\n",
    "def cal_ks(x):\n",
    "    '''\n",
    "    当前箱累计坏人数量/总坏人数量 - 当前箱累计好人数量/好人数量\n",
    "    :param x: \n",
    "    :return: \n",
    "    '''\n",
    "    ks = (x.BAD_CNT / bad_total) - (x.GOOD_CNT / good_total)\n",
    "    return round(math.fabs(ks), 3)\n",
    "\n",
    "\n",
    "'''\n",
    "从报告中可以看出:\n",
    "1、模型的KS最大值出现在第6箱(编号5),如将箱分的更细,KS值会继续增大,上限为前面通过公式计算出的KS值\n",
    "2、前4箱的样本占总人数的20%，捕捉负样本占所有负样本的56.4%，如拒绝分数最低的20%的人，可以捕捉到56.4%的负样本。\n",
    "'''\n",
    "\n",
    "report['KS'] = report.apply(cal_ks, axis=1)\n",
    "report\n",
    "\n"
   ],
   "id": "daa97372bd0b54f0",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T07:10:10.304882Z",
     "start_time": "2025-03-15T07:10:10.220291Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyecharts.charts import *\n",
    "from pyecharts import options as opts\n",
    "from pylab import *\n",
    "\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.unicode.ambiguous_as_wide', True)\n",
    "pd.set_option('display.unicode.east_asian_width', True)\n",
    "line = (\n",
    "    Line()\n",
    "    .add_xaxis(report.index.values.tolist())\n",
    "    .add_yaxis(\n",
    "        \"分组坏人占比\",\n",
    "        list(report.BADRATE),\n",
    "        yaxis_index=0,\n",
    "        color=\"red\",\n",
    "    )\n",
    "    .set_global_opts(\n",
    "        title_opts=opts.TitleOpts(title=\"评分卡模型表现\"),\n",
    "    )\n",
    "    .extend_axis(\n",
    "        yaxis=opts.AxisOpts(\n",
    "            name=\"KS值\",\n",
    "            type_=\"value\",\n",
    "            min_=0,\n",
    "            max_=0.5,\n",
    "            position=\"right\",\n",
    "            axisline_opts=opts.AxisLineOpts(\n",
    "                linestyle_opts=opts.LineStyleOpts(color=\"red\")\n",
    "            ),\n",
    "            axislabel_opts=opts.LabelOpts(formatter=\"{value}\"),\n",
    "        )\n",
    "\n",
    "    )\n",
    "    .add_yaxis(\n",
    "        \"KS\",\n",
    "        list(report['KS']),\n",
    "        yaxis_index=1,\n",
    "        color=\"blue\",\n",
    "        label_opts=opts.LabelOpts(is_show=False),\n",
    "    )\n",
    ")\n",
    "# line.render_notebook()\n",
    "line.render('./file/评分卡模型表现.html')"
   ],
   "id": "df3816d2b1b134d0",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:30:02.882896Z",
     "start_time": "2025-03-15T08:30:01.738311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def score(person_info, finance_info, credit_info, act_info):\n",
    "    '''\n",
    "    评分映射\n",
    "    :param person_info: \n",
    "    :param finance_info: \n",
    "    :param credit_info: \n",
    "    :param act_info: \n",
    "    :return: \n",
    "    '''\n",
    "    person_coefficient = 2.48386162\n",
    "    finance_coefficient = 4.44901224\n",
    "    credit_coefficient = 1.88254182\n",
    "    act_coefficient = -1.43356854\n",
    "    total_coefficient = 3.90631899\n",
    "    x_beta = person_info * person_coefficient + finance_info * finance_coefficient + credit_info * credit_coefficient + act_info * act_coefficient - total_coefficient\n",
    "    # 系数 = 基准分 + 系数 * 2^ ((1 - p)/p)\n",
    "    score = 900 - 50 * x_beta / math.log(2)\n",
    "    return score\n",
    "\n",
    "\n",
    "val['score'] = val.apply(lambda x: score(x.person_info, x.finance_info, x.credit_info, x.act_info), axis=1)\n",
    "\n",
    "fpr_lr, tpr_lr, _ = roc_curve(val_y, val['score'])\n",
    "val_ks = abs(fpr_lr - tpr_lr).max()\n",
    "print('val_ks : ', val_ks)"
   ],
   "id": "1eaac579e687f853",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T08:30:04.714358Z",
     "start_time": "2025-03-15T08:30:04.684701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 对应评级区间\n",
    "def level(score):\n",
    "    '''\n",
    "    对应评级区间\n",
    "    :param score: \n",
    "    :return: \n",
    "    '''\n",
    "    level = 0\n",
    "    if score <= 600:\n",
    "        level = 'D'\n",
    "    elif 600 < score <= 640:\n",
    "        level = 'C'\n",
    "    elif 640 < score <= 680:\n",
    "        level = 'B'\n",
    "    elif 680 < score:\n",
    "        level = 'A'\n",
    "    return level\n",
    "\n",
    "\n",
    "val['level'] = val['score'].map(lambda x: level(x))\n",
    "val.level.groupby(val.level).count() / len(val)"
   ],
   "id": "6bf4083af5dec565",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T10:00:46.587199Z",
     "start_time": "2025-03-15T10:00:46.559664Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载数据\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('./file/pima-indians-diabetes.csv')\n",
    "'''\n",
    "    数据量：768\n",
    "    字段数量：9 \n",
    "    数据类型：float64(2)、int64(7)\n",
    "    为空字段：无\n",
    "'''\n",
    "data.info()\n",
    "data.head()"
   ],
   "id": "2fe4440f22d65cb5",
   "execution_count": 22,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T10:19:35.193428Z",
     "start_time": "2025-03-15T10:19:35.086072Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加载warning日志,忽略提醒日志\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 加载python 中高性能计算库numpy\n",
    "import numpy as np\n",
    "# 加载基于numpy构建的数据计算库 pandas\n",
    "import pandas as pd\n",
    "\n",
    "# 导入序列化和反序列化\n",
    "import pickle\n",
    "\n",
    "# 导入xgboost模型\n",
    "import xgboost as xgb\n",
    "\n",
    "# 导入数据拆分库\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 导入模型加载库\n",
    "import joblib\n",
    "\n",
    "# 1、加载数据\n",
    "data = pd.read_csv('./file/Pima-Indians-Diabetes.csv')\n",
    "\n",
    "data.info()\n",
    "data.head()\n",
    "# 2、数据切分\n",
    "train, test = train_test_split(data)\n",
    "\n",
    "feature_columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI',\n",
    "                   'DiabetesPedigreeFunction', 'Age']\n",
    "target_column = 'Outcome'\n",
    "train_X = train[feature_columns].values\n",
    "train_y = train[target_column].values\n",
    "test_X = test[feature_columns].values\n",
    "test_y = test[target_column].values\n",
    "\n",
    "# 3、初始化模型\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=20, max_depth=4, learning_rate=0.1, subsample=0.7, colsample_bytree=0.7)\n",
    "\n",
    "# 4、拟合模型\n",
    "xgb_classifier.fit(train_X, train_y)\n",
    "\n",
    "# 5、使用模型预测\n",
    "predict = xgb_classifier.predict(test_X)\n",
    "\n",
    "# 6、判断准确率\n",
    "print('错误类为%f' % ((predict != test_y).sum() / float(test_y.shape[0])))\n",
    "\n",
    "# 模型存储\n",
    "joblib.dump(xgb_classifier, './file/2.model')"
   ],
   "id": "426a8727d4b1fbb5",
   "execution_count": 29,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T10:14:17.027894Z",
     "start_time": "2025-03-15T10:13:57.682277Z"
    }
   },
   "cell_type": "code",
   "source": "! pip install xgboost",
   "id": "29a0dfe209f842d3",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:03:19.526668Z",
     "start_time": "2025-03-15T11:03:14.370243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "# from xgboost.callback import EarlyStopping\n",
    "# \n",
    "# callbacks = [EarlyStopping(rounds=10)]\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "X = digits['data']\n",
    "y = digits['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "clf = xgb.XGBClassifier(\n",
    "    early_stopping_rounds=10,\n",
    "    eval_metric=\"auc\")\n",
    "clf.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "\n",
    "# callbacks = [EarlyStopping(rounds=10)]\n",
    "# clf.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"auc\", eval_set=[(X_test, y_test)])\n",
    "# clf.fit(X_train, y_train, callbacks=callbacks, eval_metric=\"auc\", eval_set=[(X_test, y_test)])\n",
    "# clf.fit(X_train, y_train,  eval_set=[(X_test, y_test)])"
   ],
   "id": "6f07fa2d08feeea5",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:04:25.662865Z",
     "start_time": "2025-03-15T11:04:24.327669Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用Xgboost输出特征重要程度\n",
    "\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "y = iris['target']\n",
    "X = iris['data']\n",
    "xgb_model = xgb.XGBClassifier().fit(X, y)\n",
    "\n",
    "temp = pd.DataFrame()\n",
    "temp['feature_names'] = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "temp['feature_importances'] = xgb_model.feature_importances_\n",
    "temp = temp.sort_values('feature_importances', ascending=False)\n",
    "temp\n"
   ],
   "id": "7f020d73ca43792f",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:05:35.299595Z",
     "start_time": "2025-03-15T11:05:33.524741Z"
    }
   },
   "cell_type": "code",
   "source": "temp.set_index('feature_names').plot.bar(figsize=(16, 8), rot=0)",
   "id": "55266b29fa17b7d4",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:06:23.112865Z",
     "start_time": "2025-03-15T11:06:22.657549Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# LightGBM\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import lightgbm as lgb\n",
    "\n",
    "data = pd.read_csv('./file/Bcard.txt')\n",
    "data.head()\n"
   ],
   "id": "30967230f65e02d5",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:06:40.814011Z",
     "start_time": "2025-03-15T11:06:40.746311Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 采用相同的方式划分测试集验证集\n",
    "df_train = data[data.obs_mth != '2018-11-30'].reset_index().copy()\n",
    "val = data[data.obs_mth == '2018-11-30'].reset_index().copy()"
   ],
   "id": "e2f13ea5b4a0f1ab",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:07:02.583392Z",
     "start_time": "2025-03-15T11:07:02.477088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用LightGBM的特征重要性以及夸时间交叉验证方式进行特征筛选 \n",
    "df_train = df_train.sort_values(by='obs_mth', ascending=False)\n",
    "df_train.head()"
   ],
   "id": "398af02c208b3322",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:07:25.739544Z",
     "start_time": "2025-03-15T11:07:25.680625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 将数据按照时间先后顺序分成5组\n",
    "df_train['rank'] = [i for i in range(df_train.shape[0])]\n",
    "df_train['rank'] = pd.cut(df_train['rank'], bins=5, labels=[i for i in range(5)])\n",
    "df_train.head()"
   ],
   "id": "a9f2a04e756ec70e",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:07:42.545782Z",
     "start_time": "2025-03-15T11:07:42.536245Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 查看分组后，每组的数据量\n",
    "df_train['rank'].value_counts()"
   ],
   "id": "27974bf5c1ce7505",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:08:04.976451Z",
     "start_time": "2025-03-15T11:08:04.969836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 查看数据总量，与每组相加结果吻合\n",
    "len(df_train)"
   ],
   "id": "6ab0aebd4e1fc284",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:11:53.303115Z",
     "start_time": "2025-03-15T11:11:43.747272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用lgbm进行分组交叉特征筛选\n",
    "lst = ['td_score', 'jxl_score', 'mj_score', 'rh_score', 'zzc_score', 'zcx_score', 'person_info',\n",
    "       'finance_info', 'credit_info', 'act_info']\n",
    "\n",
    "\n",
    "#定义lgb函数\n",
    "def LGB_test(train_x, train_y, test_x, test_y):\n",
    "    from multiprocessing import cpu_count\n",
    "    clf = lgb.LGBMClassifier(\n",
    "        boosting_type='gbdt',\n",
    "        num_leaves=31,\n",
    "        reg_alpha=0.0,\n",
    "        reg_lambda=1,\n",
    "        max_depth=2,\n",
    "        n_estimators=800,\n",
    "        max_features=140,\n",
    "        objective='binary',\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.7,\n",
    "        subsample_freq=1,\n",
    "        learning_rate=0.05,\n",
    "        min_child_weight=50,\n",
    "        random_state=None,\n",
    "        n_jobs=cpu_count() - 1,\n",
    "        early_stopping_rounds=100,\n",
    "        num_iterations=800  #迭代次数\n",
    "    )\n",
    "    clf.fit(\n",
    "        train_x,\n",
    "        train_y,\n",
    "        eval_metric='auc',\n",
    "        eval_set=[(train_x, train_y), (test_x, test_y)],\n",
    "\n",
    "    )\n",
    "    print(clf.n_features_)\n",
    "    return clf, clf.best_score_['valid_1']['auc']\n",
    "\n",
    "\n",
    "feature_lst = []\n",
    "ks_train_lst = []\n",
    "ks_test_lst = []\n",
    "for rk in set(df_train['rank']):\n",
    "    # 0，1，2，3，4，\n",
    "\n",
    "    # 1、定义模型训练集与测试集\n",
    "    t_test = df_train[df_train['rank'] == rk]\n",
    "    t_train = df_train[df_train['rank'] != rk]\n",
    "\n",
    "    train = t_train[lst]\n",
    "    train_y = t_train.bad_ind\n",
    "\n",
    "    test = t_test[lst]\n",
    "    test_y = t_test.bad_ind\n",
    "    model, auc = LGB_test(train, train_y, test, test_y)\n",
    "\n",
    "    # 2、模型贡献度放在feature中\n",
    "    feature = pd.DataFrame(\n",
    "        {'name': model.booster_.feature_name(),\n",
    "         'importance': model.feature_importances_\n",
    "         }).set_index('name')\n",
    "    feature_lst.append(feature)\n",
    "\n",
    "    # 3、计算训练集、测试集、验证集上的KS和AUC\n",
    "\n",
    "    y_predict_train_lgb = model.predict_proba(train)[:, 1]\n",
    "    y_predict_test_lgb = model.predict_proba(test)[:, 1]\n",
    "\n",
    "    train_fpr_lgb, train_tpr_lgb, _ = roc_curve(train_y, y_predict_train_lgb)\n",
    "    test_fpr_lgb, test_tpr_lgb, _ = roc_curve(test_y, y_predict_test_lgb)\n",
    "\n",
    "    train_ks = abs(train_fpr_lgb - train_tpr_lgb).max()\n",
    "    test_ks = abs(test_fpr_lgb - test_tpr_lgb).max()\n",
    "\n",
    "    train_auc = metrics.auc(train_fpr_lgb, train_tpr_lgb)\n",
    "    test_auc = metrics.auc(test_fpr_lgb, test_tpr_lgb)\n",
    "\n",
    "    ks_train_lst.append(train_ks)\n",
    "    ks_test_lst.append(test_ks)\n",
    "\n",
    "train_ks = np.mean(ks_train_lst)\n",
    "test_ks = np.mean(ks_test_lst)\n",
    "\n",
    "print('train_ks: ', train_ks)\n",
    "print('test_ks: ', test_ks)"
   ],
   "id": "d0685a42a03866d1",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:12:39.664186Z",
     "start_time": "2025-03-15T11:12:39.653518Z"
    }
   },
   "cell_type": "code",
   "source": [
    "feature_importance = pd.concat(feature_lst, axis=1).mean(1).sort_values(ascending=False)\n",
    "feature_importance[(feature_importance > 20)].index.tolist()"
   ],
   "id": "95aa760f5daf9fab",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:17:25.735271Z",
     "start_time": "2025-03-15T11:17:19.443835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 最终筛选出4个特征\n",
    "current_list = ['person_info', 'finance_info', 'credit_info', 'act_info']\n",
    "\n",
    "train = data[data.obs_mth != '2018-11-30'].reset_index().copy()\n",
    "evl = data[data.obs_mth == '2018-11-30'].reset_index().copy()\n",
    "\n",
    "x = train[current_list]\n",
    "y = train['bad_ind']\n",
    "\n",
    "evl_x = evl[current_list]\n",
    "evl_y = evl['bad_ind']\n",
    "\n",
    "model, auc = LGB_test(x, y, evl_x, evl_y)\n",
    "\n",
    "y_predict = model.predict_proba(x)[:, 1]\n",
    "fpr_lgb_train, tpr_lgb_train, _ = roc_curve(y, y_predict)\n",
    "train_ks = abs(fpr_lgb_train - tpr_lgb_train).max()\n",
    "print('train_ks : ', train_ks)\n",
    "\n",
    "y_predict = model.predict_proba(evl_x)[:, 1]\n",
    "fpr_lgb, tpr_lgb, _ = roc_curve(evl_y, y_predict)\n",
    "evl_ks = abs(fpr_lgb - tpr_lgb).max()\n",
    "print('evl_ks : ', evl_ks)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.plot(fpr_lgb_train, tpr_lgb_train, label='train LR')\n",
    "plt.plot(fpr_lgb, tpr_lgb, label='evl LR')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ],
   "id": "11805620fe22a9e7",
   "execution_count": 23,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:17:28.603529Z",
     "start_time": "2025-03-15T11:17:28.242472Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "1、从结果中看出，LightGBM效比LR要好，但LR通过一些处理，模型表现也会有提升\n",
    "2、将集成学习评分卡结果转换成分数\n",
    "'''\n",
    "\n",
    "\n",
    "def score(xbeta):\n",
    "    '''\n",
    "    好人的概率/坏人的概率\n",
    "    :param xbeta: \n",
    "    :return: \n",
    "    '''\n",
    "    score = 600 + 50 * (math.log2((1 - xbeta) / xbeta))\n",
    "    return score\n",
    "\n",
    "\n",
    "evl['xbeta'] = model.predict_proba(evl_x)[:, 1]\n",
    "evl['score'] = evl.apply(lambda x: score(x.xbeta), axis=1)\n",
    "evl['score']"
   ],
   "id": "1df40216d673c5b8",
   "execution_count": 24,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:15:15.794188Z",
     "start_time": "2025-03-15T11:15:15.781785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 用转换的分数验证KS值\n",
    "fpr, tpr, _ = roc_curve(evl_y, evl['score'])\n",
    "val_ks = abs(fpr - tpr).max()\n",
    "val_ks"
   ],
   "id": "360ddee3167f5abe",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:17:31.450476Z",
     "start_time": "2025-03-15T11:17:31.391300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 生成模型报告\n",
    "\n",
    "# 1、准备数据\n",
    "bins = 20\n",
    "temp_ = pd.DataFrame()  #创建空白DataFrame\n",
    "temp_['bad_rate_predict'] = evl['score']  # 预测结果（坏人概率）\n",
    "temp_['real_bad'] = val_y  # 真实结果\n",
    "temp_ = temp_.sort_values('bad_rate_predict')  #按照预测坏人概率降序排列\n",
    "temp_['num'] = [i for i in range(temp_.shape[0])]  #添加序号列，用于分组\n",
    "temp_['num'] = pd.cut(temp_.num, bins=bins, labels=[i for i in range(bins)])  #分成20组，为每组添加组号\n",
    "\n",
    "# 2、创建报告\n",
    "#创建空白DataFrame\n",
    "report = pd.DataFrame()\n",
    "#计算每一组坏人数量\n",
    "report['BAD'] = temp_.groupby('num').real_bad.sum().astype(int)\n",
    "#计算每一组好人数量\n",
    "report['GOOD'] = temp_.groupby('num').real_bad.count().astype(int) - report['BAD']\n",
    "#累计求和坏人数量\n",
    "report['BAD_CNT'] = report['BAD'].cumsum()\n",
    "#累计求和好人数量\n",
    "report['GOOD_CNT'] = report['GOOD'].cumsum()\n",
    "good_total = report.GOOD_CNT.max()\n",
    "bad_total = report.BAD_CNT.max()\n",
    "#计算到当前组坏人比例（占所有坏人比例）\n",
    "report['BAD_PCTG'] = round(report.BAD_CNT / bad_total, 3)\n",
    "#计算当前组坏人概率\n",
    "report['BADRATE'] = report.apply(lambda x: round(x.BAD / (x.BAD + x.GOOD), 3), axis=1)\n",
    "\n",
    "\n",
    "# 3、计算KS值\n",
    "def cal_ks(x):\n",
    "    #当前箱累计坏人数量/总坏人数量  - 当前箱累计好人数量/好人数量\n",
    "    ks = (x.BAD_CNT / bad_total) - (x.GOOD_CNT / good_total)\n",
    "    return round(math.fabs(ks), 3)\n",
    "\n",
    "\n",
    "report['KS'] = report.apply(cal_ks, axis=1)\n",
    "report"
   ],
   "id": "83cb3f93cb37b7b5",
   "execution_count": 25,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-15T11:20:20.564328Z",
     "start_time": "2025-03-15T11:20:20.511527Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# pyecharts绘图展示模型表现\n",
    "\n",
    "from pyecharts.charts import *\n",
    "from pyecharts import options as opts\n",
    "from pylab import *\n",
    "\n",
    "mpl.rcParams['font.sans-serif'] = ['SimHei']\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.unicode.ambiguous_as_wide', True)\n",
    "pd.set_option('display.unicode.east_asian_width', True)\n",
    "line = (\n",
    "\n",
    "    Line()\n",
    "    .add_xaxis(list(report.index))\n",
    "    .add_yaxis(\n",
    "        \"分组坏人占比\",\n",
    "        list(report.BADRATE),\n",
    "        yaxis_index=0,\n",
    "        color=\"red\",\n",
    "    )\n",
    "    .set_global_opts(\n",
    "        title_opts=opts.TitleOpts(title=\"评分卡模型表现\"),\n",
    "    )\n",
    "    .extend_axis(\n",
    "        yaxis=opts.AxisOpts(\n",
    "            name=\"累计坏人占比\",\n",
    "            type_=\"value\",\n",
    "            min_=0,\n",
    "            max_=0.5,\n",
    "            position=\"right\",\n",
    "            axisline_opts=opts.AxisLineOpts(\n",
    "                linestyle_opts=opts.LineStyleOpts(color=\"red\")\n",
    "            ),\n",
    "            axislabel_opts=opts.LabelOpts(formatter=\"{value}\"),\n",
    "        )\n",
    "\n",
    "    )\n",
    "    .add_yaxis(\n",
    "        \"KS\",\n",
    "        list(report['KS']),\n",
    "        yaxis_index=1,\n",
    "        color=\"blue\",\n",
    "        label_opts=opts.LabelOpts(is_show=False),\n",
    "    )\n",
    ")\n",
    "# line.render_notebook()\n",
    "line.render('./file/评分卡模型表现-LightGBM.html')"
   ],
   "id": "7b8f42859cf46d25",
   "execution_count": 27,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T12:12:46.058681Z",
     "start_time": "2025-03-18T12:12:45.973736Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 整体流程梳理\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import math\n",
    "import xgboost as xgb\n",
    "import toad\n",
    "print(np.__version__)  # 确保版本正确\n",
    "\n",
    "# 加载数据\n",
    "data_all = pd.read_csv(\"./file/scorecard.txt\")\n",
    "# data_all.info()\n",
    "# data_all.head()\n",
    "\n",
    "# 指定不参与训练列名  \n",
    "ex_lis = ['uid', 'samp_type', 'bad_ind']\n",
    "# 参与训练列名  \n",
    "ft_lis = list(data_all.columns)\n",
    "for i in ex_lis:\n",
    "    ft_lis.remove(i)\n",
    "\n",
    "# 开发样本、验证样本与时间外样本  \n",
    "dev = data_all[(data_all['samp_type'] == 'dev')]\n",
    "val = data_all[(data_all['samp_type'] == 'val')]\n",
    "off = data_all[(data_all['samp_type'] == 'off')]\n"
   ],
   "id": "623f5840111a4386",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T12:33:19.435023Z",
     "start_time": "2025-03-18T12:33:18.752702Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(np.__version__)  # 确保版本正确\n",
    "print(pd.__version__)  # 确保版本正确"
   ],
   "id": "43f8ae6ceca016f2",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T12:05:17.609115Z",
     "start_time": "2025-03-18T12:05:14.054426Z"
    }
   },
   "cell_type": "code",
   "source": "!pip list | grep -E 'numpy|pandas|scipy'",
   "id": "f37b0728a558d294",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "!pip uninstall numpy scipy pandas",
   "id": "7f0063747ef6bad1",
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T12:04:37.011111Z",
     "start_time": "2025-03-18T12:04:22.778800Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install --upgrade numpy scipy pandas",
   "id": "a0edcc3eacbf273b",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": "",
   "id": "8dd73beed738937c",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
