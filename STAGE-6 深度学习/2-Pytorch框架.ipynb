{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# PyTorch框架学习目标：\n",
    "- 知道什么是PyTorch框架，相比于TensorFlow有哪些异同点\n",
    "- PyTorch框架有哪些特点\n",
    "- PyTorch发展史有哪些重要节点\n",
    "- 学习张量，知道其基本创建方式、特点，以及基本运算、索引、转换方法\n",
    "- 学习自动微分和梯度基本计算\n",
    "- 使用PyTorch构建基本线性模型\n",
    "\n",
    "\n",
    "# 1、什么是pyTorch框架？\n",
    "概念：PyTorch是基于Python的科学计算包\n",
    "```bash\n",
    "pip install torch -i https://pypi.tuna.tsinghua.edu.cn/simple\n",
    "```\n",
    "\n",
    "# 2、PyTorch有哪些特点？\n",
    "- 类似于NumPy的张量计算：PyTorch中基本结构是张量（Tensor），它与NumPy中数组类似，但是PyTorch张量具有CPU加速的能力（通过CUDA），这使得深度学习模型能够高效的在GPU中运行\n",
    "- 自动微分系统：\n",
    "    - PyTorch提供了强大的自动微分功能（AutoGrad模块），能够自动计算模型中每个参数梯度，\n",
    "    - 自动唯粉使得梯度计算过程变得简洁高效，并且支持复杂的模型和动态计算图\n",
    "- 深度学习库：\n",
    "    - PyTorch提供了一个名为torch.nn的子模块，用于构建神经网络，它包括了大量的预构建曾层（如全联接层、卷积层、循环神经网络层），损失函数（如交叉熵、均方误差），以及优化算法（SGD、Adam等）\n",
    "    - torch.nn.module是PyTorch中构建神经网络的基础类，用户可以通过继承该类来定义自己的神经网络架构\n",
    "- 动态图计算：\n",
    "    - PyTorch使用动态计算机制，允许在运行时构建和修改模型结构，具有更高的灵活性，适合于研究人员进行实验和模型调试\n",
    "- GPU加速（CUDA支持：CUDA是NVIDIA开发的一种并行计算平台和编程模型，允许利用GPU加速计算密集型任务）：\n",
    "    - PyTorch提供对GPU的良好支持，可以轻松的将模型和数据从CPU转移到GPU或从一个GPU转移到GPU上，PyTorch回自动优化计算过程\n",
    "- 跨平台支持：\n",
    "    - PyTorch支持在多种硬件平台（如：CPU、GPU、TPU）上运行，并且支持不同的操作系统（Linux、Windows、MacOS）以及分布式计算环境（多GPU、分布式训练）\n",
    "\n",
    "# 3、PyTorch发展历史\n",
    "\n",
    "![PyTorch发展史](./file/PyTorch发展史.png)\n",
    "\n",
    "- Torch最早的是Torch框架，由Ronan·Collobert和Clement·Farabet等人开发，是一个科学计算框架，提供了多维张量操作可算计算工具\n",
    "- Torch7是Torch的一个后续版本，引入了Lua编程语言，随着pytorch的普及，Torch便不再维护，Torch7也就成为了Torch的最后一个版本。\n",
    "- Pytorch 0.1.0：是Facebook人工智能研究院（FAIR）于2016年发布了PyTorch的第一个版本，标志着PyTorch的正式诞生。\n",
    "- Pytorch 0.2.0：该版本首次引入了动态图机制，使得用户能够在构建神经网络时更加灵活。作为Pytorch后期制胜tensorflow的关键机制，该版本象征着Pytorch进入了一个新的阶段。\n",
    "- Pytorch 1.0.0：2018年发布了Pytorch的首个稳定版本，引入了Eager模式简化了模型的构建和训练过程。\n",
    "- Pytorch 2.0：Pytorch2.0引入了torch.compile，可以支持对训练过程的加速，同时引入了TorchDynamo，主要替换torch.jit.trace和torch.jit.script。另外在这个版本中编译器性能大幅提升，分布式运行方面也做了一定的优化。\n",
    "\n",
    "# 什么是张量？\n",
    "定义：张量（Tensor）是一种数学对象，也是在物理学和工程学中广泛使用的概念。它是一种多维数组，可以表示为一个n维数组，其中n是张量的阶数。张量在不同的领域有不同的应用和解释。\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "de4078022501cedb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:31:44.427407Z",
     "start_time": "2025-03-23T06:31:39.966584Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install torch -i https://pypi.tuna.tsinghua.edu.cn/simple",
   "id": "edacb791ab2b8c9",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T07:39:14.793351Z",
     "start_time": "2025-04-14T07:39:09.299683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch  # 需要安装torch模块\n",
    "import numpy as np\n",
    "\n",
    "# torch.tensor(data=, dtype=) 根据指定数据创建张量\n",
    "# 1.创建张量标量（data=, dtype=）\n",
    "data = torch.tensor(10)\n",
    "print(data)\n",
    "\n",
    "# 2.numpy数组，犹豫data为float64，张量元素类型也是float64\n",
    "data = np.random.randn(2, 3)\n",
    "data = torch.tensor(data)\n",
    "print(data)\n",
    "\n",
    "# 3.列表、浮点数默认都是flaot32\n",
    "data = [[10, 20, 30], [40, 50, 60]]\n",
    "data = torch.tensor(data)\n",
    "print(data)"
   ],
   "id": "c9ccaf69f6596041",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:32:11.100080Z",
     "start_time": "2025-03-23T06:32:11.089073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch.Tensor(size=) 根据形状来创建张量\n",
    "# 1、创建一个2行3列的张量，默认dtype为float32\n",
    "data = torch.Tensor(2, 3)\n",
    "print(data)\n",
    "\n",
    "# 2、如果传递列表代表，则包含创建包含指定元素的张量\n",
    "data = torch.Tensor([10])\n",
    "print(data)\n",
    "\n",
    "data = torch.Tensor([10, 20])\n",
    "print(data)"
   ],
   "id": "94ee1badfcd1d5a",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:32:23.434940Z",
     "start_time": "2025-03-23T06:32:23.426063Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch.IntTensor()/torch.FloatTensor() 创建指定类型的张量\n",
    "\n",
    "# 1、创建2行3列，dtype为int32的张量\n",
    "data = torch.IntTensor(2, 3)\n",
    "print(f'2行3列，dtype为int32的张量：{data}')\n",
    "\n",
    "# 2、如果传入的元素类型不正确，则回进行类型转换\n",
    "#data = torch.IntTensor([2.5, 3.3])\n",
    "#print(f'类型转换张量：{data}')\n",
    "\n",
    "# 3、其他类型\n",
    "data = torch.ShortTensor()  # int16\n",
    "print(f'int16类型张量：{data}')\n",
    "\n",
    "data = torch.LongTensor()  # int64\n",
    "print(f'int64类型张量：{data}')\n",
    "\n",
    "data = torch.FloatTensor()  # float32\n",
    "print(f'float32类型张量：{data}')\n",
    "\n",
    "data = torch.DoubleTensor()  # float64\n",
    "print(f'float64类型张量：{data}')"
   ],
   "id": "ecd7bc3be72c99fa",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:32:29.043125Z",
     "start_time": "2025-03-23T06:32:29.026188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 线性和随机张量\n",
    "\n",
    "# torch.arange(start=, end=, step=)：固定步长线性张量\n",
    "# 1、在指定区间按照补偿生成元素[start, end, steps]左闭右开\n",
    "data = torch.arange(0, 10, 2)\n",
    "print(data)\n",
    "\n",
    "# 2、在指定区间啊找元素个数生成[start, end, steps]左闭右闭\n",
    "# step = (end - start) /( step - 1)\n",
    "# value_i = start + step * i\n",
    "data = torch.linspace(0, 9, 10)\n",
    "print(data)"
   ],
   "id": "d45831a7fe5bb756",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:32:36.546413Z",
     "start_time": "2025-03-23T06:32:36.528380Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# torch.randn/rand(size=) 创建随机浮点数类型张量\n",
    "# torch.randint(low=, high=, size=) 创建随机整数类型张量 左闭右开\n",
    "# torch.initial_seed() 和 torch.manual_seed(seed=) 随机种子设置\n",
    "\n",
    "# 1、创建随机张量\n",
    "data = torch.randn(2, 3)\n",
    "print(data)\n",
    "print(f'查看随机数种子：{torch.initial_seed()}')\n",
    "\n",
    "# 2、随机数种子设置\n",
    "torch.manual_seed(100)\n",
    "data = torch.randn(2, 3)\n",
    "print(data)\n",
    "print(f'随机数种子：{torch.initial_seed()}')\n",
    "\n"
   ],
   "id": "e1ae5bef275d6fc8",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:32:43.748117Z",
     "start_time": "2025-03-23T06:32:43.725278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 指定值张量\n",
    "\n",
    "# torch.zeros(size=) 和 torch.zeros_like(input=) 创建全0张量\n",
    "# 1、创建指定形状全0张量\n",
    "data = torch.zeros(2, 3)\n",
    "print(f'创建指定形状全零张量:{data}')\n",
    "\n",
    "# 2、根据张量形状创建全0张量\n",
    "data = torch.zeros_like(data)\n",
    "print(f'根据传入张量形状创建全零张量：{data}')\n",
    "\n",
    "# torch.ones(size=) 和 torch.ones_like(input=) 创建全1张量\n",
    "# 3、创建指定形状全1张量\n",
    "data = torch.ones(2, 3)\n",
    "print(f'指定形状全一张量：{data}')\n",
    "\n",
    "data = torch.ones_like(data)\n",
    "print(f'根据输入张量形状创建全一张量：{data}')\n",
    "\n",
    "# torch.full(size=, fill_value=) 和 torch.full_like(input=, fill_value=) 创建全为指定值张量\n",
    "# 4、 创建指定形状指定值的张量\n",
    "data = torch.full([2, 3], 10)\n",
    "print(f'全为指定值张量:{data}')\n",
    "\n",
    "# 5、根据张量形状创建指定值的张量\n",
    "data = torch.full_like(data, 20)\n",
    "print(f'根据张量形状创建指定值的张量:{data}')\n"
   ],
   "id": "f8503d70defd155e",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:32:50.693795Z",
     "start_time": "2025-03-23T06:32:50.677115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 指定元素类型张量\n",
    "# data.type(dtype=)\n",
    "data = torch.full([2, 3], 10)\n",
    "print(data.dtype)\n",
    "\n",
    "# 将 data 元素类型转换为 float64 类型\n",
    "data = data.type(torch.DoubleTensor)\n",
    "print(data.dtype)\n",
    "\n",
    "# 转换为其他类型\n",
    "data = data.type(torch.ShortTensor)\n",
    "data = data.type(torch.IntTensor)\n",
    "data = data.type(torch.LongTensor)\n",
    "data = data.type(torch.FloatTensor)\n",
    "data = data.type(dtype=torch.float16)\n",
    "\n",
    "# data.half/float/double/short/int/long()\n",
    "data = torch.full([2, 3], 10)\n",
    "print(data.dtype)\n",
    "# 将 data 元素类型转换为 float64 类型\n",
    "data = data.double()\n",
    "print(data.dtype)\n",
    "# 转换为其他类型\n",
    "# data = data.short()\n",
    "# data = data.int()\n",
    "# data = data.long()\n",
    "# data = data.float()\n"
   ],
   "id": "cbc360241f2b8150",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:32:58.865841Z",
     "start_time": "2025-03-23T06:32:57.489355Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 张量类型转换\n",
    "\n",
    "# 使用 t.numpy() 函数可以将张量转换为 ndarray 数组，但是共享内存，可以使用copy函数避免共享。\n",
    "\n",
    "# 1、张量转换为NumPy数组\n",
    "data_tensor = torch.tensor([2, 3, 4])\n",
    "# 使用张量对象中的 numpy 函数进行转换\n",
    "data_numpy = data_tensor.numpy()\n",
    "print(type(data_tensor))\n",
    "print(type(data_numpy))\n",
    "# 注意: data_tensor 和 data_numpy 共享内存\n",
    "# 修改其中的一个，另外一个也会发生改变\n",
    "# data_tensor[0] = 100\n",
    "data_numpy[0] = 100\n",
    "print(data_tensor)\n",
    "print(data_numpy)\n",
    "\n",
    "# 2. 对象拷贝避免共享内存\n",
    "data_tensor = torch.tensor([2, 3, 4])\n",
    "# 使用张量对象中的 numpy 函数进行转换，通过copy方法拷贝对象\n",
    "data_numpy = data_tensor.numpy().copy()\n",
    "print(type(data_tensor))\n",
    "print(type(data_numpy))\n",
    "# 注意: data_tensor 和 data_numpy 此时不共享内存\n",
    "# 修改其中的一个，另外一个不会发生改变\n",
    "# data_tensor[0] = 100\n",
    "data_numpy[0] = 100\n",
    "print(data_tensor)\n",
    "print(data_numpy)\n"
   ],
   "id": "f4b56236b83f2901",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:33:04.983854Z",
     "start_time": "2025-03-23T06:33:04.977513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 当张量只包含一个元素时, 可以通过 item() 函数提取出该值\n",
    "data = torch.tensor([30, ])\n",
    "print(data.item())\n",
    "data = torch.tensor(30)\n",
    "print(data.item())"
   ],
   "id": "ce5266e39214d8c4",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:33:13.276277Z",
     "start_time": "2025-03-23T06:33:13.248904Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data = torch.randint(0, 10, [2, 3])\n",
    "print(f'初始化张量:{data}')\n",
    "\n",
    "# 1. 不修改原数据\n",
    "new_data = data.add(10)  # 等价 new_data = data + 10\n",
    "print(f'张量+10:{data}')\n",
    "\n",
    "# 2. 直接修改原数据 注意: 带下划线的函数为修改原数据本身\n",
    "data.add_(10)\n",
    "print(f'张量再+10:{data}')\n",
    "\n",
    "# 3. 其他函数\n",
    "print(f'张量减100:{data.sub(100)}')\n",
    "print(f'张量乘100:{data.mul(100)}')\n",
    "print(f'张量除100:{data.div(100)}')\n",
    "print(f'张量除100:{data.neg()}')"
   ],
   "id": "6cb7bd9ad995b8f5",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:33:20.690413Z",
     "start_time": "2025-03-23T06:33:20.681187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 点乘运算\n",
    "data1 = torch.tensor([[1, 2], [3, 4]])\n",
    "data2 = torch.tensor([[5, 6], [7, 8]])\n",
    "# 第一种方式\n",
    "data = torch.mul(data1, data2)\n",
    "print(data)\n",
    "# 第二种方式\n",
    "data = data1 * data2\n",
    "print(data)"
   ],
   "id": "f25ffb8a3c84c5d5",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T06:33:26.227026Z",
     "start_time": "2025-03-23T06:33:26.203973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 矩阵乘法运算\n",
    "# 点积运算\n",
    "data1 = torch.tensor([[1, 2], [3, 4], [5, 6]])\n",
    "data2 = torch.tensor([[5, 6], [7, 8]])\n",
    "# 方式一:\n",
    "data3 = data1 @ data2\n",
    "print(\"data3-->\", data3)\n",
    "# 方式二:\n",
    "data4 = torch.matmul(data1, data2)\n",
    "print(\"data4-->\", data4)"
   ],
   "id": "83c39d09dca574eb",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T10:43:35.105099Z",
     "start_time": "2025-03-23T10:43:35.094607Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "张量运算函数\n",
    "tensor.mean(dim=):平均值\n",
    "tensor.sum(dim=):求和\n",
    "tensor.min/max(dim=):最小值/最大值\n",
    "tensor.pow(exponent=):幂次方 $$x^n$$\n",
    "tensor.sqrt(dim=):平方根\n",
    "tensor.exp():指数 $$e^x$$\n",
    "tensor.log(dim=):对数 以e为底\n",
    "dim=0按列计算,dim=1按行计算\n",
    "'''\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def fundamental_operation():\n",
    "    data = torch.randint(0, 10, [2, 3], dtype=torch.float64)\n",
    "    print(data)\n",
    "    # 1. 计算均值\n",
    "    # 注意: tensor 必须为 Float 或者 Double 类型\n",
    "    print(data.mean())\n",
    "    print(data.mean(dim=0))  # 按列计算均值\n",
    "    print(data.mean(dim=1))  # 按行计算均值\n",
    "    # 2. 计算总和\n",
    "    print(data.sum())\n",
    "    print(data.sum(dim=0))\n",
    "    print(data.sum(dim=1))\n",
    "    # 3. 计算平方\n",
    "    print(torch.pow(data, 2))\n",
    "    # 4. 计算平方根\n",
    "    print(data.sqrt())\n",
    "    # 5. 指数计算, e^n 次方\n",
    "    print(data.exp())\n",
    "    # 6. 对数计算\n",
    "    print(data.log())  # 以 e 为底\n",
    "    print(data.log2())\n",
    "    print(data.log10())\n",
    "\n",
    "\n",
    "fundamental_operation()"
   ],
   "id": "8050034c006a71df",
   "execution_count": 103,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T10:42:43.765058Z",
     "start_time": "2025-03-23T10:42:43.743601Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "'''\n",
    "张量索引操作\n",
    "我们在操作张量时，经常需要去获取某些元素就进行处理或者修改操作，在这里我们需要了解在torch中的索引操作。\n",
    "'''\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def tensor_index():\n",
    "    # 随机生成数据 data[开始行(可省略默认：0): 行步长(可省略默认：1): 结束行(可省略默认：shape), 开始列(可省略默认：0): 列步长(可省略默认：1): 结束列(可省略默认：shape)]\n",
    "    data = torch.randint(0, 10, [4, 5])\n",
    "    print(f'随机生成张量：{data}')\n",
    "\n",
    "    # 1、简单行列索引\n",
    "    print(f'行索引：{data[0]}')\n",
    "    print(f'列索引：{data[:, 0]}')\n",
    "\n",
    "    # 2、列表索引\n",
    "    print(f'返回(0, 1)和(1, 2)对应位置数据：{data[[0, 1], [1, 2]]}')\n",
    "    print(f'返回0、1行的1、2列共4个元素：{data[:2, 1:3]}')\n",
    "    print(f'返回0、1行的1、2列共4个元素：{data[[[0], [1]], [1, 2]]}')\n",
    "\n",
    "    # 3、范围索引\n",
    "    print(f'前三行前两列数据：{data[:3, :2]}')\n",
    "    print(f'第二行到最后的前两列数据：{data[2:, :2]}')\n",
    "\n",
    "    # 4、布尔索引\n",
    "    print(f'第三列大于5的行数据:{data[:, :3][data[:, :3] > 5]}')\n",
    "    print(f'第二行大于5的数据：{data[:, [1]][data[:, [1]] > 5]}')\n",
    "    print(f'第二行大于5的数据：{data[:, 1:2][data[:, 1:2] > 5]}')\n",
    "\n",
    "    # 5、多维索引\n",
    "    data = torch.randint(0, 10, [3, 4, 5])\n",
    "    print(f'随机生成三维数据：{data}')\n",
    "    print(f'获取0轴上第一个数据：{data[0, :, :]}')\n",
    "    print(f'获取0轴上第一个数据：{data[[0]]}')\n",
    "\n",
    "    print(f'获取0轴上第一个数据：{data[[1]]}')\n",
    "    print(f'获取1轴上第一个数据：{data[:, 0, :]}')\n",
    "    print(f'获取2轴上第一个数据：{data[:, :, 0]}')\n",
    "\n",
    "\n",
    "tensor_index()"
   ],
   "id": "5f712379eda08afe",
   "execution_count": 100,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T10:44:24.217767Z",
     "start_time": "2025-03-23T10:44:24.176012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def reshape():\n",
    "    # 张量形状操作\n",
    "    data = torch.tensor([[10, 20, 30], [40, 50, 60]])\n",
    "    # 1. 使用 shape 属性或者 size 方法都可以获得张量的形状\n",
    "    print(data.shape, data.shape[0], data.shape[1])\n",
    "    print(data.size(), data.size(0), data.size(1))\n",
    "\n",
    "    # 2. 使用 reshape 函数修改张量形状\n",
    "    new_data = data.reshape(1, 6)\n",
    "    print(new_data.shape)\n",
    "\n",
    "    data = torch.randint(0, 10, [3, 4, 5, 6, 7])\n",
    "    print(f'张量：{data}')\n",
    "    print(f'张量形状：{data.shape}')\n",
    "    print(f'张量形状：{data.shape[0]}')\n",
    "    print(f'张量形状：{data.shape[1]}')\n",
    "    print(f'张量形状：{data.shape[2]}')\n",
    "    print(f'张量形状：{data.shape[3]}')\n",
    "\n",
    "    print(f'张量形状：{data.size(0)}')\n",
    "    print(f'张量形状：{data.size(1)}')\n",
    "    print(f'张量形状：{data.size(2)}')\n",
    "    print(f'张量形状：{data.size(3)}')\n",
    "\n",
    "    # 2. 使用 reshape 函数修改张量形状\n",
    "    reshape_data = data.reshape([3, 2, 5, 6, 14])\n",
    "    print(f'张量修改形状(总数不变)：{reshape_data}')\n",
    "    print(reshape_data.shape)\n",
    "\n",
    "    data = torch.tensor([[10, 20, 30], [40, 50, 60]])\n",
    "    # 1. 使用 shape 属性或者 size 方法都可以获得张量的形状\n",
    "    print(data.shape, data.shape[0], data.shape[1])\n",
    "    print(data.size(), data.size(0), data.size(1))\n",
    "\n",
    "    # 2. 使用 reshape 函数修改张量形状\n",
    "    new_data = data.reshape(1, 6)\n",
    "    print(new_data.shape)\n",
    "\n",
    "\n",
    "reshape()"
   ],
   "id": "80a1fb2f6f0348e3",
   "execution_count": 104,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T10:38:20.927757Z",
     "start_time": "2025-03-23T10:38:20.917757Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "张量squeeze和unsqueeze\n",
    "- squeeze:删除指定位置形状为1的维度，不指定位置删除所有形状为1的维度（降维）\n",
    "- unsqueeze:在指定位置添加形状为1的维度（升维）\n",
    "'''\n",
    "\n",
    "\n",
    "def squeeze_or_unsqueeze():\n",
    "    squeeze_data = torch.tensor([1, 2, 3, 4, 5])\n",
    "    print(f'squeeze和unsqueeze案例 维度：{squeeze_data.shape}， 数据集:{squeeze_data}')\n",
    "\n",
    "    squeeze_data = squeeze_data.unsqueeze(dim=0)\n",
    "    print(f'在位置为0的位置添加形状为1的维度 维度：{squeeze_data.shape}， 数据集:{squeeze_data}')\n",
    "\n",
    "    squeeze_data = squeeze_data.unsqueeze(dim=1)\n",
    "    print(f'在1维上拓展维度 维度：{squeeze_data.shape}， 数据集:{squeeze_data}')\n",
    "\n",
    "    squeeze_data = squeeze_data.unsqueeze(dim=-1)\n",
    "    print(f'在-1维基础上拓展维度 维度：{squeeze_data.shape}， 数据集:{squeeze_data}')\n",
    "\n",
    "\n",
    "squeeze_or_unsqueeze()"
   ],
   "id": "319cfcf29aa145d3",
   "execution_count": 97,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T10:37:47.012563Z",
     "start_time": "2025-03-23T10:37:47.001711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "transpose和permute\n",
    "transpose：实现交换正浪形状的指定维度，；例如一个形状为（2，3，4）把3和4进行交换，将张量变换为（2，4，3）\n",
    "permute：一次性交换更多的维度\n",
    "'''\n",
    "\n",
    "\n",
    "def transpose_or_permute():\n",
    "    # 创建一个三维的张量\n",
    "    # data = torch.tensor(np.random.randint(0, 10, [3, 4, 5]))\n",
    "    data = torch.randint(0, 10, [3, 4, 5])\n",
    "    print(f'data 维度：{data.shape}， 数据集:{data}')\n",
    "\n",
    "    exchange_transpose = torch.transpose(data, 1, 2)\n",
    "    print(f'交换1和2维度 形状：{exchange_transpose.shape}, 数据：{exchange_transpose}')\n",
    "\n",
    "    first_transpose = torch.transpose(data, 0, 1)\n",
    "    second_transpose = torch.transpose(first_transpose, 1, 2)\n",
    "    print(f'将形状修改为（4， 5， 3）形状：{second_transpose.shape}， 数据：{second_transpose}')\n",
    "\n",
    "    permute_data = torch.permute(data, [1, 2, 0])\n",
    "    print(f'使用permute函数将形状修改为（4， 5， 3）形状：{permute_data.shape}, 数据 ：{permute_data}')\n",
    "\n",
    "\n",
    "transpose_or_permute()"
   ],
   "id": "d43fd6448ed9d49a",
   "execution_count": 96,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T10:33:21.345790Z",
     "start_time": "2025-03-23T10:33:21.335393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "view和contiguous函数\n",
    "view：也可以用于修改张量形状，但只能用于连续张量，在PyTorch中有些张量的底层数据在内存中存储顺序与其在张量逻辑顺序不一致，view函数无反对这样的张量进行变形处理，eg：一个张量经过transpose或者permute函数的处理后，就无法使用view函数进行形状操作\n",
    "contiguous：将不连续张量转化为连续张量\n",
    "is_contiguous：判断张量是否连续，并返回True/False\n",
    "'''\n",
    "\n",
    "\n",
    "def view_or_contiguous_demo():\n",
    "    data = torch.tensor([[10, 20, 30], [40, 50, 60]])\n",
    "    print(f'shape：{data.shape}, data :{data}')\n",
    "\n",
    "    is_contiguous = data.is_contiguous()\n",
    "    print(f'张量是否连续：{is_contiguous}')\n",
    "\n",
    "    data = data.view(3, 2)\n",
    "    print(f'使用transpose改变张量形状 shape：{data.shape}, is_contiguous: {data.is_contiguous()} data:{data}')\n",
    "\n",
    "    data = torch.transpose(data, 0, 1)\n",
    "    print(f'使用transpose改变张量形状 shape：{data.shape}, is_contiguous: {data.is_contiguous()} data:{data}')\n",
    "\n",
    "    # 这里再使用view就会报错，因为当前张量不连续\n",
    "    #data = data.view(3, 2)\n",
    "    #print(f'使用view改变张量形状 shape：{data.shape}, is_contiguous: {data.is_contiguous()} data:{data}')\n",
    "\n",
    "    data = data.contiguous()\n",
    "    print(f'使用contiguous改为连续张量 shape：{data.shape}, is_contiguous: {data.is_contiguous()} data:{data}')\n",
    "\n",
    "    data = data.view(2, 3)\n",
    "    print(f'使用view改变张量形状 shape：{data.shape}, is_contiguous: {data.is_contiguous()} data:{data}')\n",
    "\n",
    "\n",
    "view_or_contiguous_demo()"
   ],
   "id": "bd259a277115ab02",
   "execution_count": 88,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T10:37:00.634315Z",
     "start_time": "2025-03-23T10:37:00.623572Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "Tensor拼接操作cat和concat\n",
    "cat:将多个张量按照指定维度拼接，要求张量在除拼接维度上其他维度保持一致\n",
    "concat:语法与cat保持一致，但更灵活，可以通过指定dim参数指定拼接维度\n",
    "stack：在一个新的维度上连接一系列张量，这回增加一个新维度，并且所有输入张量的形状必须完全相同\n",
    "备注：这里的dim可以理解为在第几层合并拼接，对应顺序位置shape增加\n",
    "'''\n",
    "\n",
    "\n",
    "def cat_demo():\n",
    "    data1 = torch.randint(0, 10, [1, 2, 3])\n",
    "    print(f'data1 shape：{data1.shape}, data :{data1}')\n",
    "    data2 = torch.randint(0, 10, [1, 2, 3])\n",
    "    print(f'data2 shape：{data2.shape}, data :{data2}')\n",
    "\n",
    "    data = torch.cat([data1, data2], dim=0)\n",
    "    print(f'data shape：{data.shape}, data :{data}')\n",
    "\n",
    "    data = torch.cat([data1, data2], dim=0)\n",
    "    print(f'data shape：{data.shape}, data :{data}')\n",
    "\n",
    "    data = torch.cat([data1, data2], dim=1)\n",
    "    print(f'data shape：{data.shape}, data :{data}')\n",
    "\n",
    "\n",
    "def stack_demo():\n",
    "    data1 = torch.randint(0, 10, [2, 3])\n",
    "    print(f'data1 shape：{data1.shape}, data :{data1}')\n",
    "    data2 = torch.randint(0, 10, [2, 3])\n",
    "    print(f'data2 shape：{data2.shape}, data :{data2}')\n",
    "\n",
    "    data = torch.stack([data1, data2], dim=0)\n",
    "    print(f'在0维上stack shape：{data.shape}, data :{data}')\n",
    "\n",
    "    data = torch.stack([data1, data2], dim=1)\n",
    "    print(f'在1维上stack shape：{data.shape}, data :{data}')\n",
    "\n",
    "    data = torch.stack([data1, data2], dim=2)\n",
    "    print(f'在2维上stack shape：{data.shape}, data :{data}')\n",
    "\n",
    "\n",
    "cat_demo()\n",
    "stack_demo()"
   ],
   "id": "f16537e1b12dfae4",
   "execution_count": 95,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T10:52:59.783360Z",
     "start_time": "2025-03-23T10:52:59.773694Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "自动微分模块：\n",
    "概念：自动微分就是自动计算梯度值,也就是计算导数\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "def scalar_gradient_calculation():\n",
    "    '''\n",
    "    标量张量梯度计算\n",
    "    :return: \n",
    "    '''\n",
    "\n",
    "    # 1、定义一个标量张量(点)\n",
    "    x = torch.tensor(10, requires_grad=True, dtype=torch.float32)\n",
    "    print(f'x shape：{x.shape}, data :{x}')\n",
    "\n",
    "    # 2、定义一个曲线\n",
    "    y = 2 * x ** 2\n",
    "\n",
    "    # 3、计算x点的梯度\n",
    "    # 此时y是一个标量,可以不用使用y.sum()转换成标量\n",
    "    # y'|(x=10) = (2*x**2)'|(x=10) = 4x|(x=10) = 40\n",
    "    # y.sum().backward()\n",
    "    y.backward()\n",
    "    print(f'x梯度：{x.grad}')\n",
    "\n",
    "\n",
    "def vector_gradient_calculation():\n",
    "    '''\n",
    "    向量张量梯度计算\n",
    "    :return: \n",
    "    '''\n",
    "    # 1、定义一个向量张量(点)\n",
    "    x = torch.tensor([10, 20], requires_grad=True, dtype=torch.float32)\n",
    "    print(f'x shape：{x.shape}, data :{x}')\n",
    "\n",
    "    # 2、定义一个曲线\n",
    "    y = 2 * x ** 2\n",
    "\n",
    "    # 计算梯度\n",
    "    # x和y都是向量张量,不能进行求导,需要将y转换成标量张量-->y.sum()\n",
    "    # y'|(x=10) = (2*x**2)'|(x=10) = 4x|(x=10) = 40\n",
    "    # y'|(x=20) = (2*x**2)'|(x=20) = 4x|(x=20) = 80\n",
    "    # 3、计算x点的梯度\n",
    "    # y.backward()\n",
    "    y.sum().backward()\n",
    "    print(f'x梯度：{x.grad}')\n",
    "\n",
    "\n",
    "scalar_gradient_calculation()\n",
    "vector_gradient_calculation()"
   ],
   "id": "1e6a24252a77660c",
   "execution_count": 111,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T11:11:47.604225Z",
     "start_time": "2025-03-23T11:11:47.451797Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    " 求 y = x**2 + 20 的极小值点 并打印y是最小值时 w的值(梯度)\n",
    " 1 定义点 x=10 requires_grad=True  dtype=torch.float32\n",
    " 2 定义函数 y = x**2 + 20\n",
    " 3 利用梯度下降法 循环迭代1000 求最优解\n",
    " 3-1 正向计算(前向传播)\n",
    " 3-2 梯度清零 x.grad.zero_()\n",
    " 3-3 反向传播\n",
    " 3-4 梯度更新 x.data = x.data - 0.01 * x.grad\n",
    "'''\n",
    "\n",
    "\n",
    "def gradient_descent_calculation():\n",
    "    # 1 定义点x=10 requires_grad=True  dtype=torch.float32\n",
    "    x = torch.tensor(10, requires_grad=True, dtype=torch.float32)\n",
    "\n",
    "    # 2 定义函数 y = x ** 2 + 20\n",
    "    y = x ** 2 + 20\n",
    "    print('开始 权重x初始值:%.6f (0.01 * x.grad):无 y:%.6f' % (x, y))\n",
    "\n",
    "    # 3 利用梯度下降法 循环迭代1000 求最优解\n",
    "    for i in range(1, 1001):\n",
    "    \n",
    "        # 3-1 正向计算(前向传播)\n",
    "        y = x ** 2 + 20\n",
    "    \n",
    "        # 3-2 梯度清零 x.grad.zero_()\n",
    "        # 默认张量的 grad 属性会累加历史梯度值 需手工清零上一次的提取\n",
    "        # 一开始梯度不存在, 需要做判断\n",
    "        if x.grad is not None:\n",
    "            x.grad.zero_()\n",
    "    \n",
    "        # 3-3 反向传播\n",
    "        y.sum().backward()\n",
    "    \n",
    "        # 3-4 梯度更新 x.data = x.data - 0.01 * x.grad\n",
    "        # x.data是修改原始x内存中的数据,前后x的内存空间一样;如果使用x,此时修改前后x的内存空间不同\n",
    "        x.data = x.data - 0.01 * x.grad  # 注：不能 x = x - 0.01 * x.grad 这样写\n",
    "    \n",
    "        print('次数:%d 权重x: %.6f, (0.01 * x.grad):%.6f y:%.6f' % (i, x, 0.01 * x.grad, y))\n",
    "    print('x：', x, x.grad, 'y最小值', y)\n",
    "\n",
    "\n",
    "def gradient_descent_calculation_error():\n",
    "    '''\n",
    "    不能将自动微分的张量转换成numpy数组，会发生报错，可以通过detach()方法实现\n",
    "    :return: \n",
    "    '''\n",
    "    # 定义一个张量\n",
    "    x1 = torch.tensor([10, 20], requires_grad=True, dtype=torch.float64)\n",
    "\n",
    "    # 将x张量转换成numpy数组\n",
    "    # 发生报错,RuntimeError: Can't call numpy() on Tensor that requires grad. Use tensor.detach().numpy() instead.\n",
    "    # 不能将自动微分的张量转换成numpy数组\n",
    "    # print(x1.numpy())\n",
    "\n",
    "    # 通过detach()方法产生一个新的张量,作为叶子结点\n",
    "    x2 = x1.detach()\n",
    "    # x1和x2张量共享数据,但是x2不会自动微分\n",
    "    print(x1.requires_grad)\n",
    "    print(x2.requires_grad)\n",
    "    # x1和x2张量的值一样,共用一份内存空间的数据\n",
    "    print(x1.data)\n",
    "    print(x2.data)\n",
    "    print(id(x1.data))\n",
    "    print(id(x2.data))\n",
    "\n",
    "    # 将x2张量转换成numpy数组\n",
    "    print(x2.detach())\n",
    "    # print(x2.numpy())\n",
    "\n",
    "gradient_descent_calculation()\n",
    "\n",
    "gradient_descent_calculation_error()\n"
   ],
   "id": "99dab4d56226efc5",
   "execution_count": 119,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T11:13:30.112455Z",
     "start_time": "2025-03-23T11:13:30.061218Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "\n",
    "def automatic_differentiation():\n",
    "    '''\n",
    "    自动微分模块应用\n",
    "    :return: \n",
    "    '''\n",
    "\n",
    "    # 输入张量 2*5\n",
    "    x = torch.ones(2, 5)\n",
    "    # 目标值是 2*3    \n",
    "    y = torch.zeros(2, 3)\n",
    "    # 设置要更新的权重和偏置的初始值\n",
    "    w = torch.randn(5, 3, requires_grad=True)\n",
    "    b = torch.randn(3, requires_grad=True)\n",
    "    # 设置网络的输出值\n",
    "    z = torch.matmul(x, w) + b  # 矩阵乘法\n",
    "    # 设置损失函数，并进行损失的计算\n",
    "    loss = torch.nn.MSELoss()\n",
    "    loss = loss(z, y)\n",
    "    # 自动微分\n",
    "    loss.backward()\n",
    "    # 打印 w,b 变量的梯度\n",
    "    # backward 函数计算的梯度值会存储在张量的 grad 变量中\n",
    "    print(\"W的梯度:\", w.grad)\n",
    "    print(\"b的梯度\", b.grad)\n",
    "\n",
    "automatic_differentiation()"
   ],
   "id": "66fb6a3bec5f4bc9",
   "execution_count": 120,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-23T11:18:21.747201Z",
     "start_time": "2025-03-23T11:18:20.694625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "'''\n",
    "我们使用 PyTorch 的各个组件来构建线性回归模型。在pytorch中进行模型构建的整个流程一般分为四个步骤：\n",
    "1、准备训练集数据\n",
    "2、构建要使用的模型\n",
    "3、设置损失函数和优化器\n",
    "4、模型训练\n",
    "\n",
    "要使用的API：\n",
    "    使用 PyTorch 的 nn.MSELoss() 代替平方损失函数\n",
    "    使用 PyTorch 的 data.DataLoader 代替数据加载器\n",
    "    使用 PyTorch 的 optim.SGD 代替优化器\n",
    "    使用 PyTorch 的 nn.Linear 代替假设函数\n",
    "\n",
    "'''\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset  # 构造数据集对象\n",
    "from torch.utils.data import DataLoader  # 数据加载器\n",
    "from torch import nn  # nn模块中有平方损失函数和假设函数\n",
    "from torch import optim  # optim模块中有优化器函数\n",
    "from sklearn.datasets import make_regression  # 创建线性回归模型数据集\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# 构造数据集\n",
    "def create_dataset():\n",
    "    x, y, coef = make_regression(n_samples=100,\n",
    "                                 n_features=1,\n",
    "                                 noise=10,\n",
    "                                 coef=True,\n",
    "                                 bias=14.5,\n",
    "                                 random_state=0)\n",
    "\n",
    "    # 将构建数据转换为张量类型\n",
    "    x = torch.tensor(x)\n",
    "    y = torch.tensor(y)\n",
    "\n",
    "    return x, y, coef\n",
    "\n",
    "# 训练模型\n",
    "def construct_liner_regression_model():\n",
    "    # 构造数据集\n",
    "    x, y, coef = create_dataset()\n",
    "\n",
    "    # 构造数据集对象\n",
    "    dataset = TensorDataset(x, y)\n",
    "\n",
    "    # 构造数据加载器\n",
    "    # dataset=:数据集对象\n",
    "    # batch_size=:批量训练样本数据\n",
    "    # shuffle=:样本数据是否进行乱序\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "    # 构造模型\n",
    "    # in_features指的是输入的二维张量的大小，即输入的[batch_size, size]中的size\n",
    "    # out_features指的是输出的二维张量的大小，即输出的[batch_size，size]中的size\n",
    "    model = nn.Linear(in_features=1, out_features=1)\n",
    "\n",
    "    # 构造平方损失函数\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # 构造优化函数\n",
    "    # params=model.parameters():训练的参数,w和b\n",
    "    # lr=1e-2:学习率, 1e-2为10的负二次方\n",
    "    print(\"w和b-->\", list(model.parameters()))\n",
    "    print(\"w-->\", model.weight)\n",
    "    print(\"b-->\", model.bias)\n",
    "    optimizer = optim.SGD(params=model.parameters(), lr=1e-2)\n",
    "\n",
    "    # 初始化训练次数\n",
    "    epochs = 100\n",
    "    # 损失的变化\n",
    "    epoch_loss = []\n",
    "    total_loss=0.0\n",
    "    train_sample=0.0\n",
    "    for _ in range(epochs):\n",
    "        for train_x, train_y in dataloader:\n",
    "            # 将一个batch的训练数据送入模型\n",
    "            y_pred = model(train_x.type(torch.float32))\n",
    "            # 计算损失值,均方误差,当前批次所有样本的平均误差 \n",
    "            loss = criterion(y_pred, train_y.reshape(-1, 1).type(torch.float32))\n",
    "            total_loss += loss.item()\n",
    "            # loss是平均误差,所以样本数+1\n",
    "            train_sample += 1\n",
    "            # 梯度清零\n",
    "            optimizer.zero_grad()\n",
    "            # 自动微分(反向传播)\n",
    "            loss.backward()\n",
    "            # 更新参数\n",
    "            optimizer.step()\n",
    "        # 计算所有batch的平均误差作为当前epoch的误差 \n",
    "        epoch_loss.append(total_loss/train_sample)\n",
    "\n",
    "    # 打印回归模型的w\n",
    "    print(model.weight)\n",
    "    # 打印回归模型的b\n",
    "    print(model.bias)\n",
    "\n",
    "    # 绘制损失变化曲线 \n",
    "    plt.plot(range(epochs), epoch_loss)\n",
    "    plt.title('损失变化曲线')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # 绘制拟合直线\n",
    "    plt.scatter(x, y)\n",
    "    x = torch.linspace(x.min(), x.max(), 1000)\n",
    "    y1 = torch.tensor([v * model.weight + model.bias for v in x])\n",
    "    y2 = torch.tensor([v * coef + 14.5 for v in x])\n",
    "    plt.plot(x, y1, label='训练')\n",
    "    plt.plot(x, y2, label='真实')\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "construct_liner_regression_model()"
   ],
   "id": "6108a7b6bbbb583d",
   "execution_count": 123,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
