{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1、什么是人工神经网络？\n",
    "人工神经网络简称ANN（Artificial Neural NetWork）是模仿生物神经网络结构和功能构建的一种计算模型。可以用于处理和学习复杂的的数据模式，尤其使用解决非线性问题。人工神经网络是机器学习中的一个重要模型，尤其是在省都学习领域中得到了广泛的应用。\n",
    "\n",
    "# 2、如何构建神经网络？\n",
    "\n",
    "![人工神经网络结构](./img/人工神经网络结构.png)\n",
    "\n",
    "对输入信息进行加权计算，并输入到下个节点做加和，再通过激活函数输出\n",
    "\n",
    "![多层人工神经网络结构](./img/多层人工神经网络结构.png)\n",
    "\n",
    "- 基本结构：\n",
    "    - 输入层（Input Layer）：即x那一层（如图像、声音、文本），每个输入藤正对应一个神经元，输入层将数据传递到下一层神经元\n",
    "    - 输出层（Output Layer）：即输出y那一层，输出层的神经元根据网络任务（回归、分类）最终生成预测结果\n",
    "    - 隐藏层（hidden layer）：输入层和输出层之间都是隐藏层，神经网络的深度“深度”通常是由隐藏层的数量决定。隐藏层的神经元通过加权激活函数处理输入，并将结果传递到下一层。\n",
    "- 特点：\n",
    "    - 同一层神经网络之间没有连接\n",
    "    - 第N层的每个神经网络和第N-1层所有的神经网络相连（Fully Connect的含义），这就是全联接层神经网络（FCNN）\n",
    "    - 全联接神经网络接收的样本数据是二维，数据在每一层之间需要以二维的形式传递\n",
    "    - 第N-1层神经元的输出就是第N层神经元的输入\n",
    "    - 每个连接都有一个权重值（w系数和b系数）\n",
    "\n",
    "\n",
    "- 内部状态值和激活值（内部状态值=加权求和值和激活值=反向传播时会产生激活值梯度和内部状态值梯度）通过控制每个神经元的内部状态值大小；每一层的内部状态值的方差、每一层的激活值的方差可让整个神经网络工作的更好。\n",
    "    - 内部状态值\n",
    "        - 神经元或隐藏单元的内部存储值，它反映了当前神经元接收到输入、理解信息以及网络内部的加权计算结果\n",
    "        - 每个输入$x_i$都有一个与之想成的权重$w_i$，表示每个输入信号的重要性\n",
    "        - $z = wx + b $(w：权重矩阵 x：输入值 b：偏置)\n",
    "    - 激活值\n",
    "        - 通过激活函数（如：ReLU、Sigmoid、Tanh）对内部状态值进行非线性变化后，得到的结果，激活值决定了当前神经元的输出。\n",
    "        - $a = f(z)$ (f：激活函数 z：内部状态值)\n",
    "    \n",
    "\n",
    "\n",
    "# 3、什么是网络非线性因素？\n",
    "- 没有引入非线性因素的网络等价于使用以恶搞线性模型拟合\n",
    "- 通过给网络输出增加激活函数，实现引入非线性因素，使得网络模型可以逼近任意函数，提升网络对复杂问题的拟合能力\n",
    "\n",
    "# 3、什么是激活函数？\n",
    "激活函数用于对于每层的输出数据进行变换，进而为了整个网络注入非线性因素，此时神经网络就可以拟合各种曲线，如果不使用激活函数整个网络虽然看起来复杂，起本质还是相当于一种线性模型\n",
    "\n",
    "\n",
    "\n",
    "- Sigmoid函数：\n",
    "    - 公式：$f(x)=\\frac{1}{1 + e^{-x}}$\n",
    "    - 将输入映射到 (0, 1) 之间，呈 S 形曲线，具有非线性特性。\n",
    "```python \n",
    "\n",
    "import torch\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "# 4、如何选择激活函数？\n",
    "\n",
    "![激活函数](./img/激活函数.png)\n",
    "\n",
    "\n",
    "对于隐藏层:\n",
    "- 优先选择ReLU激活函数\n",
    "- 如果ReLu效果不好，那么尝试其他激活，如Leaky ReLu等。\n",
    "- 如果你使用了ReLU， 需要注意一下Dead ReLU问题，避免出现0梯度从而导致过多的神经元死亡。\n",
    "- 少使用sigmoid激活函数，可以尝试使用tanh激活函数\n",
    "\n",
    "\n",
    "对于输出层:\n",
    "- 二分类问题选择sigmoid激活函数\n",
    "- 多分类问题选择softmax激活函数\n",
    "- 回归问题选择identity激活函数\n",
    "\n",
    "# 5、什么是参数初始化？\n",
    "定义：在构建网络之后，网络中的参数是需要初始化的。我们需要初始化的参数主要有权重和偏置，偏置一般初始化为0即可，而对权重的初始化则会更加重要。\n",
    "\n",
    "# 6、参数初始化作用有哪些？\n",
    "- 防止梯度消失或爆炸：初始权重过大或者过小会导致梯度在反向传播中指数级增大或缩小\n",
    "- 提到收敛速度：合理的初始化使得网络的激活值分布适中，有助于梯度高效更新\n",
    "- 保持对称性破除：权重初始化需要打破对称性，否则网络学的的能力会受到限制\n",
    "\n",
    "# 7、常见的参数初始化方法有哪些？\n",
    "\n",
    "- 随机初始化：\n",
    "    - 均匀分布初始化：权重参数舒适化从区间均居随机取数，默认区间（0，1）可以设置为-($1\\over\\sqrt{d}$$,$$1\\over\\sqrt{d}​$)均匀分布中生成当前神经元的权重，其中d为圣经元的输入数量\n",
    "    - 正太分布初始化：随机初始化从均值为0，标准差是1的高斯分布中取样，使用一些很小的值对参数w进行初始化\n",
    "    - 优点：能有效的打破对称性\n",
    "    - 缺点：随机范围不当可能会导致梯度问题\n",
    "    - 适用场景：浅层神经网络或低复杂度模型，隐藏层1～3层，总层数不超过5层\n",
    "\n",
    "- 全0初始化：神经网络中所有权重参数初始化为0\n",
    "    - 优点：实现简单\n",
    "    - 缺点：无法打破对称性，所有神经元更新方向相同，无法有效训练\n",
    "    - 适用场景：几乎不使用，仅用于偏置项的初始化    \n",
    "\n",
    "- 全1初始化：神经网络中所有权重参数初始化为1\n",
    "    - 优点：实现简单\n",
    "    - 缺点：\n",
    "        - 无法打破对称性，所有神经元是否能正常前向传播和反向传播\n",
    "        - 会导致激活值在网络中呈指数增长，容易出现梯度爆炸\n",
    "    - 适用场景：\n",
    "        - 测试或调试：比如验证神经网络是否能正常的前向传播和反向传播\n",
    "        - 特殊模型结构：某些系数网络或特定的自定义网络中可能需要手动设置部分参数为1\n",
    "        - 偏置初始化：偶尔可以讲偏置初始化为小的正值比如：0/1，但是很少用1作为偏置的初始化值\n",
    "    \n",
    "- 固定初始化：将神经网络中所有权重参数初始化为某个固定值\n",
    "    - 优点：实现简单\n",
    "    - 缺点：\n",
    "        - 无法打破对称性，所有神经元更新方向相同，无法有效的训练\n",
    "        - 初始化权重过大或者过小都可能导致梯度爆炸或消失的问题\n",
    "    - 适用场景：测试或调试\n",
    "\n",
    "\n",
    "- kaiming初始化：也叫HE初始化，转为ReLU和其变体，考虑到ReLU激活函数特性，对输入维度进行缩放\n",
    "    - 优点：适合ReLU能保持梯度稳定\n",
    "    - 缺点：对非ReLU激活函数效果一般\n",
    "    - 适用场景：深度网络10层以上，使用ReLU、Leaky ReLU激活函数\n",
    "\n",
    "- xavier初始化：也叫Glorot初始化\n",
    "    - xavier初始化分为两种：正态分布xavier初始化和均匀分布xavier初始化\n",
    "        - 正态化的Xavier初始化\n",
    "            - w权重值从均值为0, 标准差为std中随机采样，$std = sqrt(2 / (fan_in + fan_out))$\n",
    "            - std值越小，w权重值离均值0分布相对集中，计算得到的内部状态值有较小的正值或负值\n",
    "        - 均匀分布的Xavier初始化\n",
    "            - $[-limit，limit]$ 中的均匀分布中抽取样本, limit 是 $sqrt(6 / (fan_in + fan_out))$\n",
    "        - fan_in 是输入神经元个数，当前层接受的来自上一层的神经元的数量。简单来说，就是当前层接收多少个输入\n",
    "        - fan_out 是输出神经元个数，当前层输出的神经元的数量，也就是当前\n",
    "    - 优点：适用于Sigmoid、Tanh等激活函数，解决梯度消失问题\n",
    "    - 缺点：对ReLU等激活函数表现欠佳\n",
    "    - 适用场景：网络深度10层以上，使用Sigmoid和Tanh激活函数\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "643e2a94aef4fd46"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T14:39:09.409328Z",
     "start_time": "2025-05-07T14:39:09.399880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "# 添加自定义字体路径\n",
    "font_path = './font/simhei.ttf'  # 将此路径替换为你自己的 SimHei 字体文件的实际路径，也就是刚刚第二步得到的那个路径\n",
    "font_manager.fontManager.addfont(font_path)"
   ],
   "id": "28d94ec06bc5db82",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T15:19:34.702299Z",
     "start_time": "2025-05-07T15:19:34.651264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def gen_sub_plot(subplot, title, x, y):\n",
    "    '''\n",
    "        绘制子图\n",
    "    :param subplot: \n",
    "    :param title: \n",
    "    :param x: \n",
    "    :param y: \n",
    "    :return: \n",
    "    '''\n",
    "    subplot.plot(x, y)\n",
    "    subplot.grid()\n",
    "    subplot.set_title(title)\n",
    "    \n",
    "activation_fun_arr = ['Sigmoid激活函数', 'Tanh激活函数', 'ReLU激活函数', 'SoftMax激活函数']\n",
    "\n",
    "'''\n",
    "隐藏层：\n",
    "    - 优先使用ReLU函数，效果不好可以使用Leaky ReLU激活函数\n",
    "输出层：\n",
    "    - 二分类问题优先使用Sigmoid激活函数\n",
    "    - 多分类问题优先使用Softmax激活函数\n",
    "    - 回归问题优先使用identity激活函数\n",
    "'''\n",
    "\n",
    "# 创建一个绘制窗口\n",
    "# _ , axes = plt.subplots(len(activation_fun_arr), 2)\n",
    "fig, axes = plt.subplots(len(activation_fun_arr), 2, figsize=(12, 8))\n",
    "\n",
    "for index, item in enumerate(activation_fun_arr):\n",
    "    # 定义x轴数据\n",
    "    x = torch.linspace(-20, 20, 1000)\n",
    "    df_x = torch.linspace(-20, 20, 1000, requires_grad=True)\n",
    "\n",
    "    '''\n",
    "    在五层之内会出现梯度消失现象，而且该激活函数并不是以0为中心，激活总数偏向正数，导致梯度更新时候对某些特征产生相同方向影响，因此常用于二分类问题\n",
    "    '''\n",
    "    if 'Sigmoid激活函数' == item:\n",
    "        y = torch.sigmoid(x)\n",
    "        torch.sigmoid(df_x).sum().backward()\n",
    "    \n",
    "    '''\n",
    "    Tanh函数是以0为中心，并且收敛速度比Sigmoid更快，迭代次数更少，Tanh两侧导数也为0因此也会出现梯度消失问题，因此最好使用在隐藏层，在输出层使用Sigmoid函数\n",
    "    '''\n",
    "    if 'Tanh激活函数' == item:\n",
    "        y = torch.tanh(x)\n",
    "        torch.tanh(df_x).sum().backward()\n",
    "\n",
    "    '''\n",
    "    ReLu函数可以减少计算过程，ReLu不会出现梯度消失的问题因为当输入值>0梯度为1，ReLU会使一部分神经元输出是0，这样能造成网络稀疏性，并减少参数之间相互依赖关系，缓解了过拟合问题\n",
    "    '''\n",
    "    if 'ReLU激活函数' == item:\n",
    "        y = torch.relu(x)\n",
    "        torch.relu(df_x).sum().backward()\n",
    "\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    if 'SoftMax激活函数' == item:\n",
    "        '''\n",
    "        softmax用于多分类过程中，它是二分类函数sigmoid在多分类上的推广，目的是将多分类的结果以概率的形式展现出来。\n",
    "        '''\n",
    "        y = torch.softmax(x, dim=0)\n",
    "        torch.softmax(df_x, dim=0).sum().backward()  # 计算梯度\n",
    "\n",
    "    gen_sub_plot(axes[index, 0], item, x.numpy(), y.numpy())\n",
    "    # 绘制导数图像\n",
    "    if df_x.grad is not None:\n",
    "        gen_sub_plot(axes[index, 1], item + '（导数）', df_x.detach().numpy(), df_x.grad.numpy())\n",
    "    else:\n",
    "        print(f\"Warning: Gradient for {item} is None\")\n",
    "\n",
    "    # 清空梯度，避免累积\n",
    "    if df_x.grad is not None:\n",
    "        df_x.grad.zero_()\n",
    "\n",
    "plt.tight_layout()\n",
    "# 展示图像\n",
    "plt.show()"
   ],
   "id": "20725c956b2e0fc8",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "a7e6359bfc0f3ef",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
