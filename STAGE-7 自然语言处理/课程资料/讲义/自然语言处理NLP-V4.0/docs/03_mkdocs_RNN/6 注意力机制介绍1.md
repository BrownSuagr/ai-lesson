### 学习目标

- 了解什么是注意力机制的由来
- 理解什么是注意力机制
- 了解常见的注意力类型以及作业

------

## 1.  注意力机制的由来，解决了什么问题？

- 在认识注意力之前，我们先简单了解下机器翻译任务：

> 例子：seq2seq(Sequence to Sequence))架构翻译任务

<div align=center><img src="./img/atten_01.png" style="zoom:55%" ><img/></div>

- seq2seq模型架构包括三部分，分别是encoder(编码器)、decoder(解码器)、中间语义张量c。
- 图中表示的是一个中文到英文的翻译：欢迎 来 北京 → welcome to BeiJing。编码器首先处理中文输入"欢迎 来 北京"，通过GRU模型获得每个时间步的输出张量，最后将它们拼接成一个中间语义张量c；接着解码器将使用这个中间语义张量c以及每一个时间步的隐层张量, 逐个生成对应的翻译语言

- 早期在解决机器翻译这一类seq2seq问题时，通常采用的做法是利用一个编码器(Encoder)和一个解码器(Decoder)构建端到端的神经网络模型，但是基于编码解码的神经网络存在两个问题：
  - 问题1：如果翻译的句子很长很复杂，比如直接一篇文章输进去，模型的计算量很大，并且模型的准确率下降严重。
  - 问题2：在翻译时，可能在不同的语境下，同一个词具有不同的含义，但是网络对这些词向量并没有区分度，没有考虑词与词之间的相关性，导致翻译效果比较差。

- 针对这样的问题，注意力机制被提出。

------

## 2. 什么是注意力机制

- 注意力机制早在上世纪九十年代就有研究，最早注意力机制应用在视觉领域，后来伴随着2017年Transformer模型结构的提出，注意力机制在NLP,CV相关问题的模型网络设计上被广泛应用。“注意力机制”实际上就是想将人的感知方式、注意力的行为应用在机器上，让机器学会去感知数据中的重要和不重要的部分。
- 举例说明：当我们看到下面这张图时，短时间内大脑可能只对图片中的“锦江饭店”有印象，即注意力集中在了“锦江饭店”处。短时间内，大脑可能并没有注意到锦江饭店上面有一串电话号码，下面有几个行人，后面还有“喜运来大酒家”等信息。

<div align=center><img src="./img/atten_02.png" style="zoom:55%" ><img/></div>

- 所以，大脑在短时间内处理信息时，主要将图片中最吸引人注意力的部分读出来了，大脑注意力只关注吸引人的部分, 类似下图所示.

<div align=center><img src="./img/atten_03.png" style="zoom:55%" ><img/></div>

- 同样的如果我们在机器翻译中，我们要让机器注意到每个词向量之间的相关性，有侧重地进行翻译，模拟人类理解的过程。

------

## 3. 注意力机制分类以及如何实现

- 通俗来讲就是对于模型的每一个输入项，可能是图片中的不同部分，或者是语句中的某个单词分配一个权重，这个权重的大小就代表了我们希望模型对该部分一个关注程度。这样一来，通过权重大小来模拟人在处理信息的注意力的侧重，有效的提高了模型的性能，并且一定程度上降低了计算量。

- 深度学习中的注意力机制通常可分为三类: 软注意（全局注意）、硬注意（局部注意）和自注意（内注意）
  - 软注意机制(Soft/Global Attention: 对每个输入项的分配的权重为0-1之间，也就是某些部分关注的多一点，某些部分关注的少一点，因为对大部分信息都有考虑，但考虑程度不一样，所以相对来说计算量比较大。
  - 硬注意机制(Hard/Local Attention,[了解即可]): 对每个输入项分配的权重非0即1，和软注意不同，硬注意机制只考虑那部分需要关注，哪部分不关注，也就是直接舍弃掉一些不相关项。优势在于可以减少一定的时间和计算成本，但有可能丢失掉一些本应该注意的信息。
  - 自注意力机制( Self/Intra Attention): 对每个输入项分配的权重取决于输入项之间的相互作用，即通过输入项内部的"表决"来决定应该关注哪些输入项。和前两种相比，在处理很长的输入时，具有并行计算的优势。

------

### 3.1 Soft Attention (最常见)

- 需要注意：注意力机制是一种通用的思想和技术，不依赖于任何模型，换句话说，注意力机制可以用于任何模型。我们这里只是以文本处理领域的Encoder-Decoder框架为例进行理解。这里我们分别以普通Encoder-Decoder框架以及加Attention的Encoder-Decoder框架分别做对比。

------

#### 3.1.1 普通Encoder-Decoder框架

- 下图1是Encoder-Decoder框架的一种抽象表示方式：

<div align=center><img src="./img/atten_05.png" style="zoom:85%" ><img/></div>

- 上图图例可以把它看作由一个句子（或篇章）生成另外一个句子（或篇章）的通用处理模型。对于句子对<Source,Target>，我们的目标是给定输入句子Source，期待通过Encoder-Decoder框架来生成目标句子Target。Source和Target可以是同一种语言，也可以是两种不同的语言。而Source和Target分别由各自的单词序列构成：
  $$
  Source = \langle X_1,X_2 \cdots X_m \rangle \\\\
  Target = \langle y_1,y_2 \cdots y_n \rangle
  $$

- encoder顾名思义就是对输入句子Source进行编码，将输入句子通过非线性变换转化为中间语义表示C：
  $$
  C = F(X_1,X_2 \cdots X_m)
  $$

- 对于解码器Decoder来说，其任务是根据句子Source的中间语义表示C和之前已经生成的历史信息,y_1, y_2…y_i-1来生成i时刻要生成的单词y_i
  $$
  y_i = G(C,y_1,y_2 \cdots y_{i-1})
  $$

- 上述图中展示的Encoder-Decoder框架是没有体现出“注意力模型”的，所以可以把它看作是注意力不集中的分心模型。为什么说它注意力不集中呢？请观察下目标句子Target中每个单词的生成过程如下：
  $$
  y_1 = f(C) \\\\
  y_2 = f(C, y_1) \\\\
  y_3 = f(C, y_1, y_2)
  $$

- 其中f是Decoder的非线性变换函数。从这里可以看出，在生成目标句子的单词时，不论生成哪个单词，它们使用的输入句子Source的语义编码C都是一样的，没有任何区别。而语义编码C又是通过对source经过Encoder编码产生的，因此对于target中的任何一个单词，source中任意单词对某个目标单词y_i来说影响力都是相同的，这就是为什么说图1中的模型没有体现注意力的原因。

------

#### 3.1.2 加Attention的Encoder-Decoder框架

- 举例说明，为何添加Attention:
  - 比如机器翻译任务，输入source为：Tom chase Jerry，输出target为：“汤姆”，“追逐”，“杰瑞”。在翻译“Jerry”这个中文单词的时候，普通Encoder-Decoder框架中，source里的每个单词对翻译目标单词“杰瑞”贡献是相同的，很明显这里不太合理，显然“Jerry”对于翻译成“杰瑞”更重要。
  - 如果引入Attention模型，在生成“杰瑞”的时候，应该体现出英文单词对于翻译当前中文单词不同的影响程度，比如给出类似下面一个概率分布值：（Tom,0.3）(Chase,0.2) (Jerry,0.5).每个英文单词的概率代表了翻译当前单词“杰瑞”时，注意力分配模型分配给不同英文单词的注意力大小。
- 因此，基于上述例子所示, 对于target中任意一个单词都应该有对应的source中的单词的注意力分配概率.而且，由于注意力模型的加入，原来在生成target单词时候的中间语义C就不再是固定的，而是会根据注意力概率变化的C，加入了注意力模型的Encoder-Decoder框架就变成了下图2所示：

<div align=center><img src="./img/atten_06.png" style="zoom:55%" ><img/></div>

- 即生成目标句子单词的过程成了下面的形式：
  $$
  y_1 = f1(C_1) \\\\
  y_2 = f1(C_2, y_1) \\\\
  y_3 = f1(C_3, y_1, y_2)
  $$

- 而每个Ci可能对应着不同的源语句子单词的注意力分配概率分布，比如对于上面的英汉翻译来说，其对应的信息可能如下:
  $$
  C_{Tom}=g(0.6*f2(Tom), 0.2*f2(Chase), 0.2*f2(Jerry)) \\\\
  C_{Chase}=g(0.2*f2(Tom), 0.7*f2(Chase), 0.1*f2(Jerry)) \\\\
  C_{Jerry}=g(0.3*f2(Tom), 0.2*f2(Chase), 0.5*f2(Jerry))
  $$

- f2函数代表Encoder对输入英文单词的某种变换函数，比如如果Encoder是用的RNN模型的话，这个f2函数的结果往往是某个时刻输入后隐层节点的状态值；g代表Encoder根据单词的中间表示合成整个句子中间语义表示的变换函数，一般的做法中，g函数就是对构成元素加权求和，即下列公式

$$
C_i = \sum_{j=1}^{L_x}a_{ij}h_j
$$

- Lx代表输入句子source的长度, a_ij代表在Target输出第i个单词时source输入句子中的第j个单词的注意力分配系数, 而hj则是source输入句子中第j个单词的语义编码, 假设Ci下标i就是上面例子所说的'汤姆', 那么Lx就是3, h1=f('Tom'), h2=f('Chase'),h3=f('jerry')分别输入句子每个单词的语义编码, 对应的注意力模型权值则分别是0.6, 0.2, 0.2, 所以g函数本质上就是加权求和函数, 如果形象表示的话, 翻译中文单词'汤姆'的时候, 数学公式对应的中间语义表示Ci的形成过程类似下图3:

<div align=center><img src="./img/atten_07.png" style="zoom:45%" ><img/></div>

------

#### 3.1.3 如何得到注意力概率分布

- 为了便于说明，我们假设Encoder-Decoder框架中，Encoder和Decoder都采用RNN模型，如下图4所示：

<div align=center><img src="./img/atten_08.png" style="zoom:95%" ><img/></div>

- 那么注意力分配概率分布值的通用计算过程如下：

<div align=center><img src="./img/atten_09.png" style="zoom:85%" ><img/></div>

- 上图中h_i表示Source中单词j对应的隐层节点状态h_j，H_i表示Target中单词i的隐层节点状态，注意力计算的是Target中单词i对Source中每个单词对齐可能性，即F(h_j,H_i-1)，而函数F可以用不同的方法，然后函数F的输出经过softmax进行归一化就得到了注意力分配概率分布。
- 上面就是经典的Soft Attention模型的基本思想，区别只是函数F会有所不同。

------

#### 3.1.4 Attention机制的本质思想

- 其实Attention机制可以看作，Target中每个单词是对Source每个单词的加权求和，而权重是Source中每个单词对Target中每个单词的重要程度。因此，Attention的本质思想会表示成下图：

<div align=center><img src="./img/atten_10.png" style="zoom:85%" ><img/></div>

- 将Source中的构成元素看作是一系列的<Key, Value>数据对，给定Target中的某个元素Query，通过计算Query和各个Key的相似性或者相关性，即权重系数；然后对Value进行加权求和，并得到最终的Attention数值。将本质思想表示成公式如下：

<div align=center><img src="./img/atten_11.png" style="zoom:85%" ><img/></div>

- 深度学习中的注意力机制中提到：Source 中的 Key 和 Value 合二为一，指向的是同一个东西，也即输入句子中每个单词对应的语义编码，所以可能不容易看出这种能够体现本质思想的结构。因此，Attention计算转换为下面3个阶段。
- 输入由三部分构成：Query、Key和Value。其中，(Key, Value)是具有相互关联的KV对，Query是输入的“问题”，Attention可以将Query转化为与Query最相关的向量表示。
- Attention的计算主要分3步，如下图所示。

<div align=center><img src="./img/atten_12.png" style="zoom:45%" ><img/></div>

- Attention 3步计算过程Attention3步计算过程
  - 第一步：Query和Key进行相似度计算，得到Attention Score；
  - 第二步：对Attention Score进行Softmax归一化，得到权值矩阵；
  - 第三步：权重矩阵与Value进行加权求和计算。

- Query、Key和Value的含义是什么呢？我们以刚才大脑读图为例。Value可以理解为人眼视网膜对整张图片信息的原始捕捉，不受“注意力”所影响。我们可以将Value理解为像素级别的信息，那么假设只要一张图片呈现在人眼面前，图片中的像素都会被视网膜捕捉到。Key与Value相关联，Key是图片原始信息所对应的关键性提示信息，比如“锦江饭店”部分是将图片中的原始像素信息抽象为中文文字和牌匾的提示信息。一个中文读者看到这张图片时，读者大脑有意识地向图片获取信息，即发起了一次Query，Query中包含了读者的意图等信息。在一次读图过程中，Query与Key之间计算出Attention Score，得到最具有吸引力的部分，并只对具有吸引力的Value信息进行提取，反馈到大脑中。就像上面的例子中，经过大脑的注意力机制的筛选，一次Query后，大脑只关注“锦江饭店”的牌匾部分。
- 再以一个搜索引擎的检索为例。使用某个Query去搜索引擎里搜索，搜索引擎里面有好多文章，每个文章的全文可以被理解成Value；文章的关键性信息是标题，可以将标题认为是Key。搜索引擎用Query和那些文章们的标题（Key）进行匹配，看看相似度（计算Attention Score)。我们想得到跟Query相关的知识，于是用这些相似度将检索的文章Value做一个加权和，那么就得到了一个新的信息，新的信息融合了相关性强的文章们，而相关性弱的文章可能被过滤掉。

------

### 3.2 Hard Attention

- 在3.1章节我们使用了一种软性注意力的方式进行Attention机制，它通过注意力分布来加权求和融合各个输入向量。而硬性注意力（Hard Attention）机制则不是采用这种方式，它是根据注意力分布选择输入向量中的一个作为输出。这里有两种选择方式：
  - 选择注意力分布中，分数最大的那一项对应的输入向量作为Attention机制的输出。
  - 根据注意力分布进行随机采样，采样结果作为Attention机制的输出。

- 硬性注意力通过以上两种方式选择Attention的输出，这会使得最终的损失函数与注意力分布之间的函数关系不可导，导致无法使用反向传播算法训练模型，硬性注意力通常需要使用强化学习来进行训练。因此，一般深度学习算法会使用软性注意力的方式进行计算，

------

### 3.3 Self Attention

- Self Attention是Google在transformer模型中提出的，上面介绍的都是一般情况下Attention发生在Target元素Query和Source中所有元素之间。而Self Attention，指的是Source内部元素之间或者Target内部元素之间发生的Attention机制，也可以理解为Target=Source这种特殊情况下的注意力机制。当然，具体的计算过程仍然是一样的，只是计算对象发生了变化而已。
- 上面内容也有说到，一般情况下Attention本质上是Target和Source之间的一种单词对齐机制。那么如果是Self Attention机制，到底学的是哪些规律或者抽取了哪些特征呢？或者说引入Self Attention有什么增益或者好处呢？仍然以机器翻译为例来说明, 如下图所示：

<div align=center><img src="./img/atten_13.png" style="zoom:75%" ><img/></div>



- Attention的发展主要经历了两个阶段：

> - 从上图中可以看到, self Attention可以远距离的捕捉到语义层面的特征(its的指代对象是Law).
>
> - 应用传统的RNN, LSTM, 在获取长距离语义特征和结构特征的时候, 需要按照序列顺序依次计算, 距离越远的联系信息的损耗越大, 有效提取和捕获的可能性越小.
>
> - 但是应用self-attention时, 计算过程中会直接将句子中任意两个token的联系通过一个计算步骤直接联系起来

------

## 4 小结

- 学习了注意力机制的由来以及解决的问题:
  - 早期在解决机器翻译这一类seq2seq问题时，通常采用的做法是利用一个编码器(Encoder)和一个解码器(Decoder)构建端到端的神经网络模型，但是基于编码解码的神经网络存在两个问题：
    - 问题1：如果翻译的句子很长很复杂，比如直接一篇文章输进去，模型的计算量很大，并且模型的准确率下降严重。
    - 问题2：在翻译时，可能在不同的语境下，同一个词具有不同的含义，但是网络对这些词向量并没有区分度，没有考虑词与词之间的相关性，导致翻译效果比较差。
- 学习了什么是注意力机制：
  - “注意力机制”实际上就是想将人的感知方式、注意力的行为应用在机器上，让机器学会去感知数据中的重要和不重要的部分。
- 学习了不同注意力机制的类别:
  - 深度学习中的注意力机制通常可分为三类: 软注意（全局注意）、硬注意（局部注意）和自注意（内注意）
    - 软注意机制(Soft/Global Attention: 对每个输入项的分配的权重为0-1之间，也就是某些部分关注的多一点，某些部分关注的少一点，因为对大部分信息都有考虑，但考虑程度不一样，所以相对来说计算量比较大。
    - 硬注意机制(Hard/Local Attention,[了解即可]): 对每个输入项分配的权重非0即1，和软注意不同，硬注意机制只考虑那部分需要关注，哪部分不关注，也就是直接舍弃掉一些不相关项。优势在于可以减少一定的时间和计算成本，但有可能丢失掉一些本应该注意的信息。
    - 自注意力机制( Self/Intra Attention): 对每个输入项分配的权重取决于输入项之间的相互作用，即通过输入项内部的"表决"来决定应该关注哪些输入项。和前两种相比，在处理很长的输入时，具有并行计算的优势。