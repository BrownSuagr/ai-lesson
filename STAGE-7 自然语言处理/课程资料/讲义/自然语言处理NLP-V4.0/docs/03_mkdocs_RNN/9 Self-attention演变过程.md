### 学习目标

* 理解attention和self-attention之间的不同点
* 理解attention和self-attention的应用场景

## 1 Self-attention介绍

Self-attention就本质上是一种特殊的attention。这种应用在transformer中最重要的结构之一。前面我们介绍了attention机制，它能够帮我们找到子序列和全局的attention的关系，也就是找到权重值$w_i$。Self-attention向对于attention的变化，其实就是寻找权重值的$w_i$过程不同。下面我们来看看self-attention的运算过程。

* 为了能够产生输出的向量$y_i$ ，self-attention其实是对所有的输入做了一个加权平均的操作，这个公式和上面的attention是一致的。
  $$
  y_i = \sum w_{ij}x_j
  $$

* $j$代表整个序列的长度，并且$j$个权重的相加之和等于1。值得一提的是，这里的 $w_{ij}$并不是一个需要神经网络学习的参数，它是来源于$x_i$和$x_j$的之间的计算的结果（这里$w_{ij}$的计算发生了变化)。它们之间最简单的一种计算方式，就是使用点积的方式。

$$
w_{ij}^\prime = x_{i}^Tx_j
$$

> $x_i$和$x_j$是一对输入和输出。对于下一个输出的向量$y_{i+1}$，我们有一个全新的输入序列和一个不同的权重值。



* 这个点积的输出的取值范围在负无穷和正无穷之间，所以我们要使用一个$softmax$把它映射到 ![[公式]](https://www.zhihu.com/equation?tex=%5B0%2C1%5D) 之间，并且要确保它们对于整个序列而言的和为1。
  $$
  w_{ij} = \frac{exp\;w_{ij}^{\prime}}{\sum_j exp\;w_{ij}^{\prime}}
  $$

* 以上这些就是self-attention最基本的操作.

## 2 Self-attention和Attention使用方法

根据他们之间的重要区别, 可以区分在不同任务中的使用方法:

- 在神经网络中，通常来说你会有输入层（input），应用激活函数后的输出层（output），在RNN当中你会有状态（state）。如果attention (AT) 被应用在某一层的话，它更多的是被应用在输出或者是状态层上，而当我们使用self-attention（SA），这种注意力的机制更多的实在关注input上。
- Attention (AT) 经常被应用在从编码器（encoder）转换到解码器（decoder）。比如说，解码器的神经元会接受一些AT从编码层生成的输入信息。在这种情况下，AT连接的是**两个不同的组件**（component），编码器和解码器。但是如果我们用**SA**，它就不是关注的两个组件，它只是在关注你应用的**那一个组件**。那这里他就不会去关注解码器了，就比如说在Bert中，使用的情况，我们就没有解码器。
- SA可以在一个模型当中被多次的、独立的使用（比如说在Transformer中，使用了18次；在Bert当中使用12次）。但是，AT在一个模型当中经常只是被使用一次，并且起到连接两个组件的作用。
- **SA比较擅长在一个序列当中，寻找不同部分之间的关系**。比如说，在词法分析的过程中，能够帮助去理解不同词之间的关系。**AT却更擅长寻找两个序列之间的关系**，比如说在翻译任务当中，原始的文本和翻译后的文本。这里也要注意，在翻译任务重，SA也很擅长，比如说Transformer。
- AT可以连接两种不同的模态，比如说图片和文字。SA更多的是被应用在同一种模态上，但是如果一定要使用SA来做的话，也可以将不同的模态组合成一个序列，再使用SA。
- 其实有时候大部分情况，SA这种结构更加的general，在很多任务作为降维、特征表示、特征交叉等功能尝试着应用，很多时候效果都不错。

