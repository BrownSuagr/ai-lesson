### 学习目标

* 了解迁移学习中的有关概念
* 知道迁移学习的两种迁移方式

## 1 迁移学习有关概念

* 预训练模型
* 微调



### 1.1 预训练模型(Pretrained model)

* 一般情况下预训练模型都是大型模型，具备复杂的网络结构，众多的参数量，以及在足够大的数据集下进行训练而产生的模型.。在NLP领域，预训练模型往往是语言模型。因为语言模型的训练是无监督的，可以获得大规模语料，同时语言模型又是许多典型NLP任务的基础，如机器翻译，文本生成，阅读理解等，常见的预训练模型有BERT, GPT, roBERTa, transformer-XL等。



### 1.2 微调(Fine-tuning)

* 根据给定的预训练模型，改变它的部分参数或者为其新增部分输出结构后，通过在小部分数据集上训练，来使整个模型更好的适应特定任务。



### 1.3 两种迁移方式

* 直接使用预训练模型，进行相同任务的处理，不需要调整参数或模型结构，这些模型开箱即用。但是这种情况一般只适用于普适任务, 如：fasttest工具包中预训练的词向量模型。另外，很多预训练模型开发者为了达到开箱即用的效果，将模型结构分各个部分保存为不同的预训练模型，提供对应的加载方法来完成特定目标。
* 更加主流的迁移学习方式是发挥预训练模型特征抽象的能力，然后再通过微调的方式，通过训练更新小部分参数以此来适应不同的任务。这种迁移方式需要提供小部分的标注数据来进行监督学习。

* 关于迁移方式的说明:
    * 直接使用预训练模型的方式, 已经在fasttext的词向量迁移中学习。接下来的迁移学习实践将主要讲解通过微调的方式进行迁移学习。

