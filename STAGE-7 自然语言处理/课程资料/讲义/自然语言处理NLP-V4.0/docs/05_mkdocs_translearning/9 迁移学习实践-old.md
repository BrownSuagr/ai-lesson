### 学习目标

* 了解并掌握指定任务类型的微调脚本使用方法.
* 了解并掌握通过微调脚本微调后模型的使用方法.
* 掌握通过微调方式进行迁移学习的两种类型实现过程.

## 1 微调脚本介绍

指定任务类型的微调脚本:
* huggingface研究机构向我们提供了针对GLUE数据集合任务类型的微调脚本, 这些微调脚本的核心都是微调模型的最后一个全连接层.
* 通过简单的参数配置来指定GLUE中存在任务类型(如: CoLA对应文本二分类, MRPC对应句子对文本二分类, STS-B对应句子对文本多分类), 以及指定需要微调的预训练模型.



## 2 指定任务类型的微调脚本使用步骤

* 第一步: 下载微调脚本文件
* 第二步: 配置微调脚本参数
* 第三步: 运行并检验效果 



### 2.1 下载微调脚本文件

**注意：虚拟机中已经安装transformers，以下安装步骤不需再次执行**

```shell
# 克隆huggingface的transfomers文件
git clone https://github.com/huggingface/transformers.git

# 进行transformers文件夹
cd transformers

# 切换transformers到指定版本
git checkout v4.17.0

# 安装transformers包
pip install .
```

```python
# 进入微调脚本所在路径并查看
cd examples/pytorch/text-classification

ls

# 其中run_glue.py就是针对GLUE数据集合任务类型的微调脚本
```



### 2.2 配置微调脚本参数

* 在run_glue.py同级目录执行以下命令

```shell
# 定义DATA_DIR: 微调数据所在路径, 这里我们使用glue_data中的数据作为微调数据
export DATA_DIR="/root/data/glue_data"
# 定义SAVE_DIR: 模型的保存路径, 我们将模型保存在当前目录的bert_finetuning_test文件中
export SAVE_DIR="./bert_finetuning_test/"

# 使用python运行微调脚本
# --model_name_or_path: 选择具体的模型或者变体, 这里是在英文语料上微调, 因此选择bert-base-uncased
# --task_name: 它将代表对应的任务类型, 如MRPC代表句子对二分类任务
# --do_train: 使用微调脚本进行训练
# --do_eval: 使用微调脚本进行验证
# --max_seq_length: 输入句子的最大长度, 超过则截断, 不足则补齐
# --learning_rate: 学习率
# --num_train_epochs: 训练轮数
# --output_dir $SAVE_DIR: 训练后的模型保存路径
# --overwrite_output_dir: 再次训练时将清空之前的保存路径内容重新写入

# 因为空间的有限，所以虚拟机中缓存了三个模型bert-base-uncased bert-base-chinese bert-base-cased
# 因为网络原因，如果需要其他模型，需要科学上网才能下载

# 虚拟机中执行以下命令耗时较长，建议在有GPU的主机上执行

# 模型1，该命令已在虚拟机执行，再次执行会覆盖缓存的模型
python run_glue.py \
  --model_name_or_path bert-base-uncased \
  --task_name mrpc \
  --do_train \
  --do_eval \
  --max_seq_length 128 \
  --learning_rate 2e-5 \
  --num_train_epochs 1.0 \
  --output_dir bert-base-uncased-finetuning \
  --overwrite_output_dir


# 模型2，该命令已在虚拟机执行，再次执行会覆盖缓存的模型
python run_glue.py \
  --model_name_or_path bert-base-chinese \
  --task_name mrpc \
  --do_train \
  --do_eval \
  --max_seq_length 128 \
  --learning_rate 2e-5 \
  --num_train_epochs 1.0 \
  --output_dir bert-base-chinese-finetuning \
  --overwrite_output_dir

# 模型3，该命令已在虚拟机执行，再次执行会覆盖缓存的模型
python run_glue.py \
  --model_name_or_path bert-base-cased \
  --task_name mrpc \
  --do_train \
  --do_eval \
  --max_seq_length 128 \
  --learning_rate 2e-5 \
  --num_train_epochs 1.0 \
  --output_dir bert-base-cased-finetuning \
  --overwrite_output_dir

```



### 2.3 检验效果

#### 1 输出效果

```text
# 最终打印模型的验证结果:
01/05/2020 23:59:53 - INFO - __main__ -   Saving features into cached file ../../glue_data/MRPC/cached_dev_bert-base-uncased_128_mrpc
01/05/2020 23:59:53 - INFO - __main__ -   ***** Running evaluation  *****
01/05/2020 23:59:53 - INFO - __main__ -     Num examples = 408
01/05/2020 23:59:53 - INFO - __main__ -     Batch size = 8
Evaluating: 100%|█| 51/51 [00:23<00:00,  2.20it/s]
01/06/2020 00:00:16 - INFO - __main__ -   ***** Eval results  *****
01/06/2020 00:00:16 - INFO - __main__ -     acc = 0.7671568627450981
01/06/2020 00:00:16 - INFO - __main__ -     acc_and_f1 = 0.8073344506341863
01/06/2020 00:00:16 - INFO - __main__ -     f1 = 0.8475120385232745
```

#### 2 查看文件内容

```text
added_tokens.json  
checkpoint-450  
checkpoint-400  
checkpoint-350  
checkpoint-200  
checkpoint-300  
checkpoint-250  
checkpoint-200  
checkpoint-150  
checkpoint-100  
checkpoint-50     
pytorch_model.bin        
training_args.bin
config.json       
special_tokens_map.json  
vocab.txt
eval_results.txt  
tokenizer_config.json
```

#### 3 文件说明

* pytorch_model.bin代表模型参数，可以使用torch.load加载查看；
* traning_args.bin代表模型训练时的超参，如batch_size，epoch等，仍可使用torch.load查看；
* config.json是模型配置文件，如多头注意力的头数，编码器的层数等，代表典型的模型结构，如bert，xlnet，一般不更改；
* added_token.json记录在训练时通过代码添加的自定义token对应的数值，即在代码中使用add_token方法添加的自定义词汇；
* special_token_map.json当添加的token具有特殊含义时，如分隔符，该文件存储特殊字符的及其对应的含义，使文本中出现的特殊字符先映射成其含义，之后特殊字符的含义仍然使用add_token方法映射。 
* checkpoint: 若干步骤保存的模型参数文件(也叫检测点文件)。



### 2.4 使用本地微调模型

```python
import torch
from transformers import AutoModel, AutoTokenizer

# 1 通过git clone下模型包, 然后再使用
# 2 直接本地加载模型
mypathname = '/root/transformers/examples/pytorch/text-classification/bert-base-uncased-finetuning'
tokenizer = AutoTokenizer.from_pretrained(mypathname)
model = AutoModel.from_pretrained(mypathname)

index = tokenizer.encode("Talk is cheap", "Please show me your code!")
# 102是bert模型中的间隔(结束)符号的数值映射
mark = 102

# 找到第一个102的索引, 即句子对的间隔符号
k = index.index(mark)

# 句子对分割id列表, 由0，1组成, 0的位置代表第一个句子, 1的位置代表第二个句子
segments_ids = [0]*(k + 1) + [1]*(len(index) - k - 1)
# 转化为tensor
tokens_tensor = torch.tensor([index])
segments_tensors = torch.tensor([segments_ids])

# 使用评估模式
with torch.no_grad():
    # 使用模型预测获得结果
    result = model(tokens_tensor, token_type_ids=segments_tensors)
    # 打印预测结果以及张量尺寸
    print(result)
    print(result[0].shape)
```



> * 输出效果:

```text
(tensor([[[-0.1591,  0.0816,  0.4366,  ...,  0.0307, -0.0419,  0.3326],
         [-0.3387, -0.0445,  0.9261,  ..., -0.0232, -0.0023,  0.2407],
         [-0.0427, -0.1688,  0.5533,  ..., -0.1092,  0.1071,  0.4287],
         ...,
         [-0.1800, -0.3889, -0.1001,  ..., -0.1369,  0.0469,  0.9429],
         [-0.2970, -0.0023,  0.1976,  ...,  0.3776, -0.0069,  0.2029],
         [ 0.7061,  0.0102, -0.4738,  ...,  0.2246, -0.7604, -0.2503]]]), tensor([[-3.5925e-01,  2.0294e-02, -2.3487e-01,  4.5763e-01, -6.1821e-02,
          2.4697e-02,  3.8172e-01, -1.8212e-01,  3.4533e-01, -9.7177e-01,
          1.1063e-01,  7.8944e-02,  8.2582e-01,  1.9020e-01,  6.5513e-01,
         -1.8114e-01,  3.9617e-02, -5.6230e-02,  1.5207e-01, -3.2552e-01,
          ...
          1.4417e-01,  3.0337e-01, -6.6146e-01, -9.6959e-02,  8.9790e-02,
          1.2345e-01, -5.9831e-02,  2.2399e-01,  8.2549e-02,  6.7749e-01,
          1.4473e-01,  5.4490e-01,  5.9272e-01,  3.4453e-01, -8.9982e-02,
         -1.2631e-01, -1.9465e-01,  6.5992e-01]]))
torch.Size([1, 12, 768])
```



## 3 通过微调方式进行迁移学习的两种类型
* 类型一: 使用指定任务类型的微调脚本微调预训练模型, 后接带有输出头的预定义网络输出结果.
* 类型二: 直接加载预训练模型进行输入文本的特征表示, 后接自定义网络进行微调输出结果. 

* 说明: 所有类型的实战演示, 都将针对中文文本进行. 

### 3.1 类型一实战演示

#### 1 介绍

* 使用文本二分类的任务类型SST-2的微调脚本微调中文预训练模型, 后接带有分类输出头的预定义网络输出结果. 目标是判断句子的情感倾向.
* 准备中文酒店评论的情感分析语料, 语料样式与SST-2数据集相同, 标签0代表差评, 标签1好评.
* 语料存放在与glue_data/同级目录cn_data/下, 其中的SST-2目录包含train.tsv和dev.tsv

* train.tsv

```text
sentence	label
早餐不好,服务不到位,晚餐无西餐,早餐晚餐相同,房间条件不好,餐厅不分吸烟区.房间不分有无烟房.	0
去的时候 ,酒店大厅和餐厅在装修,感觉大厅有点挤.由于餐厅装修本来该享受的早饭,也没有享受(他们是8点开始每个房间送,但是我时间来不及了)不过前台服务员态度好!	1
有很长时间没有在西藏大厦住了，以前去北京在这里住的较多。这次住进来发现换了液晶电视，但网络不是很好，他们自己说是收费的原因造成的。其它还好。	1
非常好的地理位置，住的是豪华海景房，打开窗户就可以看见栈桥和海景。记得很早以前也住过，现在重新装修了。总的来说比较满意，以后还会住	1
交通很方便，房间小了一点，但是干净整洁，很有香港的特色，性价比较高，推荐一下哦	1
酒店的装修比较陈旧，房间的隔音，主要是卫生间的隔音非常差，只能算是一般的	0
酒店有点旧，房间比较小，但酒店的位子不错，就在海边，可以直接去游泳。8楼的海景打开窗户就是海。如果想住在热闹的地带，这里不是一个很好的选择，不过威海城市真的比较小，打车还是相当便宜的。晚上酒店门口出租车比较少。	1
位置很好，走路到文庙、清凉寺5分钟都用不了，周边公交车很多很方便，就是出租车不太爱去（老城区路窄爱堵车），因为是老宾馆所以设施要陈旧些，	1
酒店设备一般，套房里卧室的不能上网，要到客厅去。	0

```

* dev.tsv

```text
sentence	label
房间里有电脑，虽然房间的条件略显简陋，但环境、服务还有饭菜都还是很不错的。如果下次去无锡，我还是会选择这里的。	1
我们是5月1日通过携程网入住的，条件是太差了，根本达不到四星级的标准，所有的东西都很陈旧，卫生间水龙头用完竟关不上，浴缸的漆面都掉了，估计是十年前的四星级吧，总之下次是不会入住了。	0
离火车站很近很方便。住在东楼标间，相比较在九江住的另一家酒店，房间比较大。卫生间设施略旧。服务还好。10元中式早餐也不错，很丰富，居然还有青菜肉片汤。	1
坐落在香港的老城区，可以体验香港居民生活，门口交通很方便，如果时间不紧，坐叮当车很好呀！周围有很多小餐馆，早餐就在中远后面的南北嚼吃的，东西很不错。我们定的大床房，挺安静的，总体来说不错。前台结账没有银联！	1
酒店前台服务差，对待客人不热情。号称携程没有预定。感觉是客人在求他们，我们一定得住。这样的宾馆下次不会入住！	0
价格确实比较高，而且还没有早餐提供。	1
是一家很实惠的酒店，交通方便，房间也宽敞，晚上没有电话骚扰，住了两次，有一次住５０１房间，洗澡间排水不畅通，也许是个别问题．服务质量很好，刚入住时没有调好宽带，服务员很快就帮忙解决了．	1
位置非常好，就在西街的街口，但是却闹中取静，环境很清新优雅。	1
房间应该超出30平米,是HK同级酒店中少有的大;重装之后,设备也不错.	1
```

#### 2 运行代码

在run_glue.py同级目录执行以下命令

```shell
# 使用python运行微调脚本
# --model_name_or_path: 选择bert-base-chinese
# --task_name: 句子二分类任务sst2
# --do_train: 使用微调脚本进行训练
# --do_eval: 使用微调脚本进行验证
# --max_seq_length: 128，输入句子的最大长度

# 该命令已在虚拟机执行，再次执行会覆盖缓存的模型

python run_glue.py \
  --model_name_or_path bert-base-chinese \
  --task_name sst2 \
  --do_train \
  --do_eval \
  --max_seq_length 128 \
  --learning_rate 2e-5 \
  --num_train_epochs 1.0 \
  --output_dir bert-base-chinese-sst2-finetuning
```

> * 检验效果

```text
# 最终打印模型的验证结果, 准确率高达0.88.
01/06/2020 14:22:36 - INFO - __main__ -   Saving features into cached file ../../cn_data/SST-2/cached_dev_bert-base-chinese_128_sst-2
01/06/2020 14:22:36 - INFO - __main__ -   ***** Running evaluation  *****
01/06/2020 14:22:36 - INFO - __main__ -     Num examples = 1000
01/06/2020 14:22:36 - INFO - __main__ -     Batch size = 8
Evaluating: 100%|████████████| 125/125 [00:56<00:00,  2.20it/s]
01/06/2020 14:23:33 - INFO - __main__ -   ***** Eval results  *****
01/06/2020 14:23:33 - INFO - __main__ -     acc = 0.88
```

#### 3 查看文件内容:

```text
added_tokens.json
checkpoint-350
checkpoint-200
checkpoint-300
checkpoint-250
checkpoint-200
checkpoint-150
checkpoint-100
checkpoint-50
pytorch_model.bin
training_args.bin
config.json
special_tokens_map.json
vocab.txt
eval_results.txt
tokenizer_config.json
```



#### 4 使用本地微调模型

```python
import torch
# 0 找到自己预训练模型的路径
mymodelname = '/root/transformers/examples/pytorch/text-classification/bert-base-chinese-sst2-finetuning'
print(mymodelname)

# 1 本地加载预训练模型的tokenizer
tokenizer = AutoTokenizer.from_pretrained(mymodelname)

# 2 本地加载 预训练模型 带分类模型头
model = AutoModelForSequenceClassification.from_pretrained(mymodelname)
# model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)

# 3 默认情况下 加载预训练模型，不带头
# model = AutoModel.from_pretrained('./transformers/examples/pytorch/text-classification/bert_finetuning_test_hug')

text = "早餐不好,服务不到位,晚餐无西餐,早餐晚餐相同,房间条件不好"
index = tokenizer.encode(text)
tokens_tensor = torch.tensor([index])

# 使用评估模式
with torch.no_grad():
    # 使用模型预测获得结果
    result = model(tokens_tensor)
    print(result[0])

predicted_label = torch.argmax(result[0]).item()
print('预测标签为>', predicted_label)

text1 = "房间应该超出30平米,是HK同级酒店中少有的大;重装之后,设备也不错."
index = tokenizer.encode(text1)
tokens_tensor = torch.tensor([index])

# 使用评估模式
with torch.no_grad():
    # 使用模型预测获得结果
    result = model(tokens_tensor)
    print(result[0])

predicted_label = torch.argmax(result[0]).item()
print('预测标签为>', predicted_label)

```

> * 输出效果:

```text
输入文本为: 早餐不好,服务不到位,晚餐无西餐,早餐晚餐相同,房间条件不好
预测标签为: 0

输入文本为: 房间应该超出30平米,是HK同级酒店中少有的大;重装之后,设备也不错.
预测标签为: 1
```



### 3.2 类型二实战演示

#### 1 介绍

* 直接加载预训练模型进行输入文本的特征表示, 后接自定义网络进行微调输出结果. 
* 使用语料和完成的目标与类型一实战相同.



#### 2 加载预训练模型

直接加载预训练模型进行输入文本的特征表示:

```python
import torch
# 进行句子的截断补齐(规范长度)
from keras.preprocessing import sequence

# 因为空间原因，虚拟机中之缓存了huggingface/pytorch-transformers模型

# 从本地加载
source = '/root/.cache/torch/hub/huggingface_pytorch-transformers_master'
# 从github加载
# source = 'huggingface/pytorch-transformers'

# 直接使用预训练的bert中文模型
model_name = 'bert-base-chinese'

# 通过torch.hub获得已经训练好的bert-base-chinese模型
model =  torch.hub.load(source, 'model', model_name, source='local')
# 从github加载
# model =  torch.hub.load(source, 'model', model_name, source='github')

# 获得对应的字符映射器, 它将把中文的每个字映射成一个数字
tokenizer = torch.hub.load(source, 'tokenizer', model_name, source='local')
# 从github加载
# tokenizer = torch.hub.load(source, 'tokenizer', model_name, source='github')

# 句子规范长度
cutlen = 32

def get_bert_encode(text):
    """
    description: 使用bert-chinese编码中文文本
    :param text: 要进行编码的文本
    :return: 使用bert编码后的文本张量表示
    """
    # 首先使用字符映射器对每个汉字进行映射
    # 这里需要注意, bert的tokenizer映射后会为结果前后添加开始和结束标记即101和102 
    # 这对于多段文本的编码是有意义的, 但在我们这里没有意义, 因此使用[1:-1]对头和尾进行切片
    indexed_tokens = tokenizer.encode(text[:cutlen])[1:-1]
    # 对映射后的句子进行截断补齐
    indexed_tokens = sequence.pad_sequences([indexed_tokens], cutlen) 
    # 之后将列表结构转化为tensor
    tokens_tensor = torch.LongTensor(indexed_tokens)
    # 使模型不自动计算梯度
    with torch.no_grad():
        # 调用模型获得隐层输出
        encoded_layers = model(tokens_tensor)
    # 输出的隐层是一个三维张量, 最外层一维是1, 我们使用[0]降去它.
    encoded_layers = encoded_layers[0]
    return encoded_layers

```

> * 调用:

```python
if __name__ == "__main__":
    text = "早餐不好,服务不到位,晚餐无西餐,早餐晚餐相同,房间条件不好"
    encoded_layers = get_bert_encode(text)
    print(encoded_layers)
    print(encoded_layers.shape)
```

> * 输出效果:

```text
tensor([[[-0.4078,  0.8188, -0.6263,  ..., -0.0878, -0.3879,  0.1973],
         [-0.1980,  0.4741,  0.1832,  ...,  0.1118, -0.2924,  0.0820],
         [ 0.6442,  0.7331, -1.0680,  ...,  0.2806, -0.1484,  0.7688],
         ...,
         [ 1.2418, -0.0812, -0.3268,  ...,  1.0782,  0.1485, -1.1028],
         [ 0.2462, -0.5323,  0.0962,  ..., -0.8405,  0.8222, -0.1156],
         [ 0.6589, -0.0304, -0.7150,  ..., -0.4237,  0.3504, -0.7093]]])
torch.Size([1, 32, 768])
```



#### 3 自定义单层的全连接网络

自定义单层的全连接网络作为微调网络。根据实际经验, 自定义的微调网络参数总数应大于0.5倍的训练数据量, 小于10倍的训练数据量, 这样有助于模型在合理的时间范围内收敛.

```python
import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    """定义微调网络的类"""
    def __init__(self, char_size=32, embedding_size=768):
        """
        :param char_size: 输入句子中的字符数量, 即输入句子规范后的长度128.
        :param embedding_size: 字嵌入的维度, 因为使用的bert中文模型嵌入维度是768, 因此embedding_size为768
        """
        super(Net, self).__init__()
        # 将char_size和embedding_size传入其中
        self.char_size = char_size
        self.embedding_size = embedding_size
        # 实例化一个全连接层
        self.fc1 = nn.Linear(char_size*embedding_size, 2)

    def forward(self, x):
        # 对输入的张量形状进行变换, 以满足接下来层的输入要求
        x = x.view(-1, self.char_size*self.embedding_size)
        # 使用一个全连接层
        x = self.fc1(x)
        return x
```



> * 调用:

```python
if __name__ == "__main__":
    # 随机初始化一个输入参数
    x = torch.randn(1, 32, 768)
    # 实例化网络结构, 所有参数使用默认值
    net = Net()
    nr = net(x)
    print(nr)    
```



> * 输出效果:

```text
tensor([[0.3279, 0.2519]], grad_fn=<ReluBackward0>)
```



#### 4 构建训练与验证数据批次生成器

```python
import pandas as pd
from collections import Counter
from functools import reduce
from sklearn.utils import shuffle

def data_loader(train_data_path, valid_data_path, batch_size):
    """
    description: 从持久化文件中加载数据
    :param train_data_path: 训练数据路径
    :param valid_data_path: 验证数据路径
    :param batch_size: 训练和验证数据集的批次大小
    :return: 训练数据生成器, 验证数据生成器, 训练数据数量, 验证数据数量
    """
    # 使用pd进行csv数据的读取, 并去除第一行的列名
    train_data = pd.read_csv(train_data_path, header=None, sep="\t").drop([0])
    valid_data = pd.read_csv(valid_data_path, header=None, sep="\t").drop([0])

    # 打印训练集和验证集上的正负样本数量
    print("训练数据集的正负样本数量:")
    print(dict(Counter(train_data[1].values)))
    print("验证数据集的正负样本数量:")
    print(dict(Counter(valid_data[1].values)))

    # 验证数据集中的数据总数至少能够满足一个批次
    if len(valid_data) < batch_size:
        raise("Batch size or split not match!")

    def _loader_generator(data):
        """
        description: 获得训练集/验证集的每个批次数据的生成器
        :param data: 训练数据或验证数据
        :return: 一个批次的训练数据或验证数据的生成器
        """
        # 以每个批次的间隔遍历数据集
        for batch in range(0, len(data), batch_size):
            # 定义batch数据的张量列表
            batch_encoded = []
            batch_labels = []
            # 将一个bitch_size大小的数据转换成列表形式, 并进行逐条遍历
            for item in shuffle(data.values.tolist())[batch: batch+batch_size]:
                # 使用bert中文模型进行编码
                encoded = get_bert_encode(item[0])
                # 将编码后的每条数据装进预先定义好的列表中
                batch_encoded.append(encoded)
                # 同样将对应的该batch的标签装进labels列表中
                batch_labels.append([int(item[1])])
            # 使用reduce高阶函数将列表中的数据转换成模型需要的张量形式
            # encoded的形状是(batch_size*max_len, embedding_size)
            encoded = reduce(lambda x, y: torch.cat((x, y), dim=0), batch_encoded)
            labels = torch.tensor(reduce(lambda x, y: x + y, batch_labels))
            # 以生成器的方式返回数据和标签
            yield (encoded, labels)

    # 对训练集和验证集分别使用_loader_generator函数, 返回对应的生成器
    # 最后还要返回训练集和验证集的样本数量
    return _loader_generator(train_data), _loader_generator(valid_data), len(train_data), len(valid_data)
```



> * 调用:

```python
if __name__ == "__main__":
    train_data_path = "/root/data/glue_data/SST-2/train.tsv"
    valid_data_path = "/root/data/glue_data/SST-2/dev.tsv"
    batch_size = 16
    train_data_labels, valid_data_labels, \
    train_data_len, valid_data_len = data_loader(train_data_path, valid_data_path, batch_size)
    print(next(train_data_labels))
    print(next(valid_data_labels))
    print("train_data_len:", train_data_len)
    print("valid_data_len:", valid_data_len)
```




> * 输出效果:

```text
训练数据集的正负样本数量:
{'0': 29780, '1': 37569}
验证数据集的正负样本数量:
{'1': 444, '0': 428}
(tensor([[[-0.6303,  1.1318, -0.3418,  ...,  1.6460, -0.1171,  0.7541],
         [-0.5715,  0.9577, -0.4190,  ...,  1.5169, -0.0387,  0.6166],
         [-0.5301,  0.7905, -0.2580,  ...,  1.5954, -0.0559,  0.6453],
         ...,
         [-0.3087,  0.6281,  0.1010,  ...,  1.5620, -0.1870,  0.5816],
         [-0.2482,  0.6478,  0.0386,  ...,  1.4672, -0.2018,  0.6288],
         [ 0.0115,  0.8074,  0.3172,  ...,  1.8373, -0.0368,  0.5223]],
        ...,

        [[-0.7761,  1.2271, -0.1928,  ...,  1.3955, -0.4057,  0.7237],
         [-0.6987,  1.2270, -0.2225,  ...,  1.4247, -0.3673,  0.6321],
         [-0.6177,  1.0689, -0.0544,  ...,  1.5243, -0.4109,  0.6564],
         ...,
         [-0.2122,  0.7630, -0.1084,  ...,  1.5221, -0.0703,  0.4527],
         [-0.5035,  0.7712, -0.2957,  ...,  1.4507, -0.1208,  0.5033],
         [-0.3215,  0.7201, -0.0899,  ...,  1.4875, -0.1781,  0.6034]]]), tensor([1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0]))
train_data_len: 67349
valid_data_len: 872
```



#### 5 编写训练和验证函数

```python
import torch.optim as optim

def train(train_data_labels):
    """
    description: 训练函数, 在这个过程中将更新模型参数, 并收集准确率和损失
    :param train_data_labels: 训练数据和标签的生成器对象
    :return: 整个训练过程的平均损失之和以及正确标签的累加数
    """
    # 定义训练过程的初始损失和准确率累加数
    train_running_loss = 0.0
    train_running_acc = 0.0
    # 循环遍历训练数据和标签生成器, 每个批次更新一次模型参数
    for train_tensor, train_labels in train_data_labels:
        # 初始化该批次的优化器
        optimizer.zero_grad()
        # 使用微调网络获得输出
        train_outputs = net(train_tensor)
        # 得到该批次下的平均损失
        train_loss = criterion(train_outputs, train_labels)
        # 将该批次的平均损失加到train_running_loss中
        train_running_loss += train_loss.item()
        # 损失反向传播
        train_loss.backward()
        # 优化器更新模型参数
        optimizer.step()
        # 将该批次中正确的标签数量进行累加, 以便之后计算准确率
        train_running_acc += (train_outputs.argmax(1) == train_labels).sum().item()
    return train_running_loss, train_running_acc


def valid(valid_data_labels):
    """
    description: 验证函数, 在这个过程中将验证模型的在新数据集上的标签, 收集损失和准确率
    :param valid_data_labels: 验证数据和标签的生成器对象
    :return: 整个验证过程的平均损失之和以及正确标签的累加数
    """
    # 定义训练过程的初始损失和准确率累加数
    valid_running_loss = 0.0
    valid_running_acc = 0.0
    # 循环遍历验证数据和标签生成器
    for valid_tensor, valid_labels in valid_data_labels:
        # 不自动更新梯度
        with torch.no_grad():
            # 使用微调网络获得输出
            valid_outputs = net(valid_tensor)
            # 得到该批次下的平均损失
            valid_loss = criterion(valid_outputs, valid_labels)
            # 将该批次的平均损失加到valid_running_loss中
            valid_running_loss += valid_loss.item()
            # 将该批次中正确的标签数量进行累加, 以便之后计算准确率
            valid_running_acc += (valid_outputs.argmax(1) == valid_labels).sum().item()
    return valid_running_loss,  valid_running_acc

```

#### 6 调用并保存模型

```python
if __name__ == "__main__":
    # 设定数据路径
    train_data_path = "/root/data/glue_data/SST-2/train.tsv"
    valid_data_path = "/root/data/glue_data/SST-2/dev.tsv"
    # 定义交叉熵损失函数
    criterion = nn.CrossEntropyLoss()
    # 定义SGD优化方法
    optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
    # 定义训练轮数
    epochs = 4
    # 定义批次样本数量
    batch_size = 16
    # 进行指定轮次的训练
    for epoch in range(epochs):
        # 打印轮次
        print("Epoch:", epoch + 1)
        # 通过数据加载器获得训练数据和验证数据生成器, 以及对应的样本数量
        train_data_labels, valid_data_labels, train_data_len, \
        valid_data_len = data_loader(train_data_path, valid_data_path, batch_size)
        # 调用训练函数进行训练
        train_running_loss, train_running_acc = train(train_data_labels)
        # 调用验证函数进行验证
        valid_running_loss, valid_running_acc = valid(valid_data_labels)
        # 计算每一轮的平均损失, train_running_loss和valid_running_loss是每个批次的平均损失之和
        # 因此将它们乘以batch_size就得到了该轮的总损失, 除以样本数即该轮次的平均损失
        train_average_loss = train_running_loss * batch_size / train_data_len
        valid_average_loss = valid_running_loss * batch_size / valid_data_len
    
        # train_running_acc和valid_running_acc是每个批次的正确标签累加和,
        # 因此只需除以对应样本总数即是该轮次的准确率
        train_average_acc = train_running_acc /  train_data_len
        valid_average_acc = valid_running_acc / valid_data_len
        # 打印该轮次下的训练损失和准确率以及验证损失和准确率
        print("Train Loss:", train_average_loss, "|", "Train Acc:", train_average_acc)
        print("Valid Loss:", valid_average_loss, "|", "Valid Acc:", valid_average_acc)
    
    print('Finished Training')
    
    # 保存路径
    MODEL_PATH = './BERT_net.pth'
    # 保存模型参数
    torch.save(net.state_dict(), MODEL_PATH) 
    print('Finished Saving')    

```



> * 输出效果:

```text
Epoch: 1
Train Loss: 2.144986984236597 | Train Acc: 0.7347972972972973
Valid Loss: 2.1898122818128902 | Valid Acc: 0.704
Epoch: 2
Train Loss: 1.3592962406135032 | Train Acc: 0.8435810810810811
Valid Loss: 1.8816152956699324 | Valid Acc: 0.784
Epoch: 3
Train Loss: 1.5507876996199943 | Train Acc: 0.8439189189189189
Valid Loss: 1.8626576719331536 | Valid Acc: 0.795
Epoch: 4
Train Loss: 0.7825378059198299 | Train Acc: 0.9081081081081082
Valid Loss: 2.121698483480899 | Valid Acc: 0.803
Finished Training
Finished Saving
```



#### 7 加载模型进行使用

```python
if __name__ == "__main__":
    MODEL_PATH = './BERT_net.pth'
    # 加载模型参数
    net.load_state_dict(torch.load(MODEL_PATH))
    
    # text = "酒店设备一般，套房里卧室的不能上网，要到客厅去。"
    text = "房间应该超出30平米,是HK同级酒店中少有的大;重装之后,设备也不错."
    print("输入文本为:", text)
    with torch.no_grad():
        output = net(get_bert_encode(text))
        # 从output中取出最大值对应的索引
        print("预测标签为:", torch.argmax(output).item())

```

> * 输出效果:

```text
输入文本为: 房间应该超出30平米,是HK同级酒店中少有的大;重装之后,设备也不错.
预测标签为: 1
输入文本为: 酒店设备一般，套房里卧室的不能上网，要到客厅去。
预测标签为: 0
```



## 4 小结

* 学习了指定任务类型的微调脚本:
	* huggingface研究机构向我们提供了针对GLUE数据集合任务类型的微调脚本, 这些微调脚本的核心都是微调模型的最后一个全连接层.
	* 通过简单的参数配置来指定GLUE中存在任务类型(如: CoLA对应文本二分类, MRPC对应句子对文本二分类, STS-B对应句子对文本多分类), 以及指定需要微调的预训练模型.

* 学习了指定任务类型的微调脚本使用步骤:
	* 第一步: 下载微调脚本文件
	* 第二步: 配置微调脚本参数
	* 第三步: 运行并检验效果

* 学习了通过微调方式进行迁移学习的两种类型:
  * 类型一: 使用指定任务类型的微调脚本微调预训练模型, 后接带有输出头的预定义网络输出结果.
  * 类型二: 直接加载预训练模型进行输入文本的特征表示, 后接自定义网络进行微调输出结果.

* 学习了类型一实战演示:
  * 使用文本二分类的任务类型SST-2的微调脚本微调中文预训练模型, 后接带有分类输出头的预定义网络输出结果. 目标是判断句子的情感倾向.
  * 准备中文酒店评论的情感分析语料, 语料样式与SST-2数据集相同, 标签0代表差评, 标签1好评.



* 学习了类型二实战演示:
	* 直接加载预训练模型进行输入文本的特征表示, 后接自定义网络进行微调输出结果.

