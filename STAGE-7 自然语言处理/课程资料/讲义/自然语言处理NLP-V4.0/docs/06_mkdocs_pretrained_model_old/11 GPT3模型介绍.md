### 学习目标

- 认识GPT-3模型
- 了解GPT-3模型的相关细节内容
- 了解Chat-GPT模型与GPT-3模型的区别和联系
- 掌握对Chat_GPT模型API接口的使用

## 1 GPT-3的介绍

GPT-3 (Generative Pre-training Transformer 3) 是由OpenAI开发的一种大型自然语言生成模型，具有非常强大的自然语言生成能力，可以生成高质量的自然语言文本。GPT-3能够执行许多自然语言处理任务，如翻译、问答、摘要生成、文本分类等。

GPT-3于2020年5月早些时候由Open AI推出，作为其先前语言模型 (LM) GPT-2 的继承者。 它被认为比GPT-2更好、更大。事实上，与他语言模型相比，OpenAI GPT-3 的完整版拥有大约 1750 亿个可训练参数，是迄今为止训练的最大模型，这份 72 页的[研究论文](https://arxiv.org/pdf/2005.14165.pdf) 非常详细地描述了该模型的特性、功能、性能和局限性。

下图为不同模型之间训练参数的对比：

<div align=center><img src="./img/image-20230102113422181.png" style="zoom:50%" ><img/></div>

## 2 GPT-3的模型细节

### 2.1 GPT-3训练数据集

一般来说，模型的参数越多，训练模型所需的数据就越多。GPT-3共训练了5个不同的语料大约 45 TB 的文本数据，分别是低质量的Common Crawl，高质量的WebText2，Books1，Books2和Wikipedia，GPT-3根据数据集的不同的质量赋予了不同的权值，权值越高的在训练的时候越容易抽样到，如下表所示。

|       **数据集**        | **数量（tokens）** | **训练数据占比** |
| :---------------------: | :----------------: | :--------------: |
| Common Crawl（filterd） |       4100亿       |       60%        |
|        Web Text2        |       190亿        |       22%        |
|          BOOK1          |       120亿        |        8%        |
|          BOOK2          |       550亿        |        8%        |
|        Wikipedia        |        30亿        |        2%        |

不同数据的介绍：

- Common Crawl语料库包含在 8 年的网络爬行中收集的 PB 级数据。语料库包含原始网页数据、元数据提取和带有光过滤的文本提取。
- WebText2是来自具有 3+ upvotes 的帖子的所有出站 Reddit 链接的网页文本。
- Books1和Books2是两个基于互联网的图书语料库。
- 英文维基百科页面 也是训练语料库的一部分。

### 2.2 GPT-3模型架构

GPT-3 不是一个单一的模型，而是一个模型系列。系列中的每个模型都有不同数量的可训练参数。下表显示了每个模型、体系结构及其对应的参数：

<div align=center><img src="./img/image-20230102114040173.png" style="zoom:50%" ><img/></div>

事实上，OpenAI GPT-3 系列模型与 GPT-2 模型相架构完全一致。

最大版本 GPT-3 175B 或“GPT-3”具有175个B参数、96层的多头Transformer、Head size为96、词向量维度为12288、文本长度大小为2048。

### 2.3 GPT-3三种评估方式

任何语言模型可以执行的各种任务取决于它是如何微调/更新的。使用 GPT-3，可以完成前面讨论的许多 NLP 任务，而无需任何微调、梯度或参数更新，这使得该模型**与任务无关**。因此，OpenAI GPT-3 可以在很少或没有示例的情况下执行任务。让我们了解与模型相关的Few-shot/One-shot/Zero-shot任务概念，并通过一些示例了解如何与模型进行交互。

Few-shot、One-shot、Zero-shot Learning策略主要是用于解决神经网络模型因为训练数据少，导致模型泛化能力差的问题。

以从英语到法语的翻译任务为例，分别对比传统的微调策略和GPT-3三种评估方式。

下图是传统的微调策略：

<div align=center><img src="./img/image-20230102164042027.png" style="zoom:65%" ><img/></div>

传统的微调策略存在问题：

- 微调需要对每一个任务有一个任务相关的数据集以及和任务相关的微调。
- 需要一个相关任务大的数据集，而且需要对其进行标注
- 当一个样本没有出现在数据分布的时候，泛化性不见得比小模型要好

下图显示了 GPT-3 三种评估方式:

<div align=center><img src="./img/image-20230102163953793.png" style="zoom:65%" ><img/></div>

在zero-shot的设置条件下：先给出任务的描述，之后给出一个测试数据对其进行测试，直接让预训练好的模型去进行任务测试。

在one-shot的设置条件下：在预训练和真正翻译的样本之间，插入一个样本做指导。好比说在预训练好的结果和所要执行的任务之间，给一个例子，告诉模型英语翻译为法语，应该这么翻译。

在few-shot的设置条件下：在预训练和真正翻译的样本之间，插入多个样本做指导。好比说在预训练好的结果和所要执行的任务之间，给多个例子，告诉模型应该如何工作。

<div align=center><img src="./img/image-20230102153847399.png" style="zoom:60%" ><img/></div>

上述表格显示了 GPT-3 模型在执行零样本、单样本和少样本翻译任务时的Blue分数对比（用于度量同一源语句的自动翻译与人工创建的参考翻译之间的差异）。从表中可以看出， GPT-3 三种评估方式不用微调、梯度或参数更新，模型依然可以达到很好的效果，其中GPT-3的few-shot还在部分任务上超越了当前SOTA。

## 3 GPT-3和ChatGPT的区别和联系

ChatGPT是一种基于GPT-3的聊天机器人模型。它旨在使用 GPT-3 的语言生成能力来与用户进行自然语言对话。例如，用户可以向 ChatGPT 发送消息，然后 ChatGPT 会根据消息生成一条回复。

GPT-3 是一个更大的自然语言处理模型，而 ChatGPT 则是使用 GPT-3 来构建的聊天机器人。它们之间的关系是 ChatGPT 依赖于 GPT-3 的语言生成能力来进行对话。

## 4 python调用ChatGPT模型

要使用GPT-3或ChatGPT模型，您需要先访问OpenAI的API网站(https://beta.openai.com/docs/quickstart)，然后从API网站获取你的 API 密钥，然后则可以使用Python调用GPT-3或ChatGPT模型。

首先，您需要安装OpenAI的Python库，可以使用以下命令完成：

```python
pip install openai
```

要使用 ChatGPT 模型，您需要使用以下代码：

```python
import openai

openai.api_key = "YOUR API KEY"

model_engine = "text-davinci-002"
# 使用GPT3: model_engine ="davinci"
prompt = "Hi, how are you doing today?"

completions = openai.Completion.create(
    engine=model_engine,
    prompt=prompt,
    max_tokens=1024,
    n=1,
    stop=None,
    temperature=0.7,
)

message = completions.choices[0].text
print(message)
```

请注意，上述代码仅是示例，您可能需要根据自己的需要调整代码以获得所需的结果

## 5 小结

- 学习了什么是GPT-3:
  - GPT-3 是由OpenAI开发的一种大型自然语言生成模型，是迄今为止训练的最大模型，可以生成高质量的自然语言文本。GPT-3能够执行许多自然语言处理任务，如翻译、问答、摘要生成、文本分类。

  - GPT3是在GPT2基础上发展处的更强大的语言预训练模型.

- 了解了GPT3的工作细节:
  - GPT-3共训练了5个不同的语料大约 45 TB 的文本数据
  - OpenAI GPT-3系列模型与GPT-2模型相架构完全一致。最大版本GPT-3具有1750 亿个可训练参数、96层的多头Transformer、Head size为96、词向量维度为12288、文本长度大小为2048。
  - Few-shot、One-shot、Zero-shot Learning策略主要是用于解决神经网络模型因为训练数据少，导致模型泛化能力差的问题。

- 学习了ChatGPT和GPT3的区别和联系:

  - ChatGPT是一种基于GPT-3的聊天机器人模型。它旨在使用GPT-3的语言生成能力来与用户进行自然语言对话。例如，用户可以向ChatGPT 发送消息，然后ChatGPT会根据消息生成一条回复。

  - GPT-3是一个更大的自然语言处理模型，而ChatGPT则是使用GPT-3来构建的聊天机器人。它们之间的关系是ChatGPT依赖于GPT-3的语言生成能力来进行对话。

- 学习了如何用python调用ChatGPT模型:
  - pip install openai

