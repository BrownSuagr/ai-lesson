## 机器学习阶段回归回顾

有了问题之后, 判断用分类, 回归, 聚类 解决问题

哪些能用来解决分类问题

哪些能用来解决回归问题



朴素贝叶斯, SVM, Adaboost, GBDT  KMeans LinearRegression, 



逻辑回归 （风控 ， 推荐最后一步）

决策树（cart树） 随机森林 XGBoost 

KNN 算法不太常用 但是思想还是比较常用的, 通过计算距离, 离得近就是相似的。



XGBoost  逻辑回归、决策树（森林）

逻辑回归： 二分类问题， 在具体实践中很多，可解释性强

XGBoost： 既可以分类， 也可以回归，可解释性稍差（单条数据不太好解释），精度比较好

- 模型无关可解释 SHAP 

模型的可解释  ：  逻辑回顾和决策树， 可解释性模型自带的





### KNN

基本流程

归一化、标准化

训练集测试集划分

交叉验证网格搜索 

分类问题评估准确率

### 线性回归

损失函数/ 梯度下降/ 正则化  L1正则 L2正则  ☆☆☆☆☆

线性回归的解法/优化方法:  正规方程 梯度下降

回归问题的评估   ☆☆☆☆☆

过拟合和欠拟合的概念以及解决方法  ☆☆☆☆☆



### 逻辑回归

逻辑回归的基本原理

- sigmoid 函数
- 对数似然损失
- 二分类问题更多的时候要用的是预测的概率 而不是最终的 0/1 标签, 要根据概率的高低对样本进行排序

分类问题的评估(样本不均衡的时候)

混淆矩阵:  精准率 召回率  F1Score

ROC/AUC    知道ROC是怎么画出来的   ☆☆☆☆☆

- TPR  所有的1标签中  被预测为1的比例
- FPR   所有的0标签中 被预测为1的比例
- AUC >0.65 勉强能说的过去的结果   0.7左右才会考虑上线



### 决策树

ID3 C4.5决策树  了解

- 信息增益   ID3  类别取值比较多的特征, 计算出较大的信息增益概率比较高, ID3决策树选择取值较多的类别倾向较大
- 信息增益率  C4.5 
- 多少分叉都可以

cart 树

- 二叉树
- 分类问题:  基尼指数
  - 叶子结点所有样本投票就是预测值
- 回归问题: 平方损失
  - 叶子结点所有样本的平均值 就是预测值

### 聚类算法-KMeans

聚类基本原理

- 先确定K的类中心点(随机), 计算其它数据点距离中心点的距离找到每个数据点所属的类别,  计算每一个类别的均值, 找到新的聚类中心点, 如果新的聚类中心点和之前的一致,聚类结束, 否则重新计算聚类。 

如何评估聚类的好坏

肘方法

sc系数

业务角度 是否能得到启发聚类之后, 某个字段在不同的类别中, 有明确的边界



### 集成学习

bagging的基本思想 (又放回的采样)

- 样本采样
- 特征采样
- 平权投票
- 随机森林( n棵cart树)

boosting的基本思想  串行训练多个模型, 逐步逼近最优结果

Adaboost  通过调整样本权重跟模型权重 来逐步降低模型误差, 所有模型在一起投票

- 每一轮训练之后, 都会将分错的样本权重提升, 分对的样本权重降低
- 最终投票的时候, 错误率低的模型权重高, 错误率高的模型权重低

GBDT → XGBoost  串行训练  每一个新的模型 fit 上一轮的误差

- gbdt  xgboost  梯度提升  gradient boosting

- 梯度提升 跟梯度下降基本是一个思路

  - 梯度下降 模型参数的迭代优化过程  每迭代一次 通过参数w的调整 逐步逼近损失的最小值

  -  gradient boosting  每一次迭代, 多添加一个模型, 多添加的这个模型的目标就是使整体的损失减小

    也就是说,  训练的第 N个模型是朝着 低N-1个模型 目标函数的梯度下降方向去训练的

- XGBoost 跟 GBDT 的区别

  - 损失函数中考虑了树的复杂度  $\large \Omega(f_t) = \gamma T + \frac{1}{2}\lambda \sum_{j=1}^{T}w_j^2$
  - 对损失函数做了二阶泰勒级数展开
  - 训练每一颗树的时候, 并行执行的, 效率比GBDT高
    - XGBoost 跟 GBDT 原理一样 都是一个模型一个模型训练的



### 朴素贝叶斯

贝叶斯定理

特征独立假设  P(AB) = P(A) P(B)

拉普拉斯平滑系数

朴素贝叶斯使用场景   文本处理  情感分类  垃圾邮件处理

### SVM

概念  支撑向量 margin  超平面

LinearSVC

SVC

LinearSVR

SVR





### EDA 和 特征工程

- 目的 找到跟目标有关系的数据 , 这个数据可能就是原始的数据, 也有可能需要经过加工处理之后, 才会跟目标有关系
- 将数据进行加工处理, 从原始数据→ 新的数据   特征衍生 (升维)



最后会总结出一些特征衍生的套路, 可以按照套路无差别的先搞出很多新的特征来, 按照一些套路,做特征筛选, 把作用不大的, 线性相关的特征, 筛选掉(降维) 

**特征衍生套路**  (可解释)

对数值型特征取对数, 开根号

添加多项式特征

特征交叉   性别 年龄交叉 

**特征筛选(降维)**  (可解释) 

看特征之间的相关性, 相关性强的 二选一

单个特征看方差, 方差小的删除

目标跟特征之间是否有关联  没什么关联的删除 只保留有关联的

集成学习 特征重要性 

(PCA 可解释性不好  L1 不好控制)





数值型特征,  归一化/标准化  

类别型特征  one-hot 编码  LabelEncoder



数据挖掘领域中  可解释性放在第一位的, 精度放在第二位



机器学习模型 从数据中找规律

特征工程作用

①  把包含规律的数据找到,  把不包含规律的数据丢掉

② 把包含规律的数据处理成方便算法计算的形式

归一化 标准化 类别数据的编码 

**EDA**   探索数据 给特征工程提供思路

- 要盯着目标去做  

- 回归问题 连续目标

  看相关性 ( 连续特征和连续性特征)

  类别型特征 (目标值按类别画kde图, 对比分布是否有区别)

- 分类问题 类别目标

  分类的目标跟连续特征( KDE, 对比分布是否有区别) 

  分类的目标 跟 分类特征 ( 斯皮尔曼相关系数 等级)

  分类的目标 跟 分类特征 画分类柱状图 统计不同标签下, 不同类别数量是否有差别

**数据清洗**

- 数据类型问题
- 缺失值 高缺失率删除
  - 低缺失率 可以先带着做EDA,  根据数据的理解情况再 决定如何填充缺失值
- 异常值  3* iqr   iqr =  3/4分位数 -1/4 分位数
- 重复值  删掉



**用户画像**

- 清楚用户画像, 做标签的作用, 方便运营的时候, 通过不同的角度去精准的圈人

AB测试

- 经过了离线测试,  算法正式上线之前, 先经过AB测试
- AB测试的目的, 少量用户先体验, 降低由于设计误差导致的用户体验变差, 让少部分用户代表全部用户做实验
- 通过实验结果, 决定到底要不要上线
  - 推荐系统  之前 推荐点击率 8% ~10%
  - 计算参与实验的最少人数 给用户打上不同的标签, 开始实验
  - 收集数据 计算点击率 如果达到了10% 
- 假设检验 
  - 原假设
  - 备择假设
  - α 5%   β 20%