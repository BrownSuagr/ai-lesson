{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc83e131b005b99e",
   "metadata": {},
   "source": [
    "# 梯度下降法学习目标\n",
    "- 掌握梯度下降算法原理\n",
    "- 掌握梯度下降算法优化损失函数的原理\n",
    "\n",
    "--- \n",
    "\n",
    "# 1、梯度的概念\n",
    "- 在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；\n",
    "- 在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向；\n",
    "- 在微积分里面，对多元函数的参数求∂偏导数，把求得的各个参数的偏导数以向量的形式写出来，就是梯度。\n",
    "\n",
    "# 2、梯度下降公式\n",
    "\n",
    "![梯度下降公式](../img/梯度下降公式.png)\n",
    "\n",
    "\n",
    "# 3、什么是梯度下降优化原理？\n",
    "![梯度下降优化原理](../img/梯度下降优化原理.png)\n",
    "\n",
    "\n",
    "# 4、小结\n",
    "- 梯度下降法(gradient descent)是一个最优化算法，常用于机器学习和深度学习中用来递归性地逼近最小偏差模型\n",
    "- 梯度下降法的计算过程就是沿梯度下降的方向求解极小值（也可以沿梯度上升方向求解极大值）\n",
    "- 线性回归的回归系数可以通过梯度下降算法找到损失函数的极小值得到\n",
    "- 梯度下降中，学习率（Learning rate）是一个很重要的参数，它决定了在梯度下降迭代的过程中，每一步沿梯度负方向前进的长度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3966d162a9f6acf5",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
