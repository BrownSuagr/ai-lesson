{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90f406697a68758e",
   "metadata": {},
   "source": [
    "# 集成学习的概述学习目标：\n",
    "> 集成学习时解决有监督及其学习任务的一类方法，它的思路是基于多个学习算法的集成来的提升预测结果，在实际的场景中应用非常广泛\n",
    "- 集成学习的一些思考\n",
    "- Bagging和随机森林算法\n",
    "- AdaBoost算法\n",
    "- GBDT算法\n",
    "- XGBoost算法\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "# 集成学习的学习目标：\n",
    "- 知道什么是集成学习\n",
    "- 知道集成学习的分类\n",
    "---\n",
    "\n",
    "# 1、为什么学习集成学习？\n",
    "概念：集成学习是机器学习中一种思想，通过多个多个模型组合成一个精度更高的模型，通过多个弱学习联合进行预测\n",
    "\n",
    "# 2、什么是集成学习？\n",
    "- 传统机器学习算法：决策树、逻辑回归都是寻找一个最优解\n",
    "- 集成算法：算法基本思想就是将多个分类器组合，从而实现一个预测结果更好的集成分类器。集成学习通过建立建立几个模型来解决单一预测问题，它的工作原理就是生成多个分类器模型，各自独立学习作出预测，这些预测结合成预测结果，因此优于任意一个单分类器作出预测。\n",
    "\n",
    "# 3、什么是“基学习器”？\n",
    "参与组成集成学习的模型称为弱学习器也叫“基学习器”\n",
    "\n",
    "# 4、集成学习有哪些分类？\n",
    "概念：‌集成学习可以分为同质和异质两种类型，其中同质集成学习是指所有基学习器属于同一个种类，而异质集成学习则是指所有的个体学习器不全是一个种类的。\n",
    "\n",
    "\n",
    "# 5、集成学习有哪些算法？\n",
    "关键词：“三个臭皮匠胜于诸葛亮”\n",
    "- Bagging（Bootstrap Aggregating）\n",
    "    - 概念：并行训练模型的方法。创建多个相同类型的模型，每个模型在独立的数据子集上进行训练。降低方差\n",
    "    - 常见算法：随机森林（Random Forest）\n",
    "- Boosting\n",
    "    - 概念：顺序【串行】训练模型的方法。创建多个相同类型的模型，基于先前模型的表现来调整后续模型的预测能力。降低偏差\n",
    "    - 常见算法：AdaBoost、Gradient Boosting、XGBoost、LightGBM等\n",
    "- Stacking：\n",
    "    - 概念：并行训练模型的方法。创建多个不同类型的模型，每个模型在独立的数据子集上进行训练。\n",
    "    - 常见方法：\n",
    "- Voting：\n",
    "    - 概念：Voting方法是一种将多个基本学习器的预测结果进行投票或平均来获得最终预测的简单方法。硬投票是基于多数票决定，而软投票是基于概率得分的平均\n",
    "    - 常见方法：硬投票（Hard Voting）、软投票（Soft Voting）\n",
    "        - \n",
    "![集成学习对比](../img/集成学习对比.png)\n",
    "\n",
    "\n",
    "\n",
    "# 6、集成学习性能如何评估？\n",
    "‌集成学习的性能可以通过多种方法进行评估，主要包括交叉验证法、概率图模型和调参搜索法。\n",
    "- ‌交叉验证法：将数据集划分为K份，选择一份作为测试集，其他作为训练集，对比全部模型的训练结果，确定最佳的模型\n",
    "- 概率图模型：通过构建概率图模型和模型之间依赖关系，直观的确定判断模型之间的差异和关系，‌但对于多变量情况效果不佳。\n",
    "- 调参搜索法：通过手动或者自动的形式选取参数值，通过遍历不同的参数组合获取性能最好的模型。\n",
    "\n",
    "# 小结\n",
    "- “基学习器”使用的学习方法是相同的还是不同的？可以是相同的（多个基学习器都使用决策树）也可以是不同的（比如：支持向量机、神经网络、决策树整合到一起作为一个集成学习系统），一般倾向于相同学习模型\n",
    "- “基学习器”应该注意哪些？基学习器之间要存在差异性；基学习器能力不需要很强，只要比随机猜测0.5高一点就行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af2aea7b641bef6d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T11:30:45.334981Z",
     "start_time": "2024-08-31T11:30:45.315111Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鸢尾花数据集描述：.. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n",
      "参数： 600 150\n",
      "训练集： 480 120\n",
      "测试集： 120 30\n",
      "模型预测原始数据：x_test:[[6.1 2.8 4.7 1.2]\n",
      " [5.7 3.8 1.7 0.3]\n",
      " [7.7 2.6 6.9 2.3]\n",
      " [6.  2.9 4.5 1.5]\n",
      " [6.8 2.8 4.8 1.4]\n",
      " [5.4 3.4 1.5 0.4]\n",
      " [5.6 2.9 3.6 1.3]\n",
      " [6.9 3.1 5.1 2.3]\n",
      " [6.2 2.2 4.5 1.5]\n",
      " [5.8 2.7 3.9 1.2]\n",
      " [6.5 3.2 5.1 2. ]\n",
      " [4.8 3.  1.4 0.1]\n",
      " [5.5 3.5 1.3 0.2]\n",
      " [4.9 3.1 1.5 0.1]\n",
      " [5.1 3.8 1.5 0.3]\n",
      " [6.3 3.3 4.7 1.6]\n",
      " [6.5 3.  5.8 2.2]\n",
      " [5.6 2.5 3.9 1.1]\n",
      " [5.7 2.8 4.5 1.3]\n",
      " [6.4 2.8 5.6 2.2]\n",
      " [4.7 3.2 1.6 0.2]\n",
      " [6.1 3.  4.9 1.8]\n",
      " [5.  3.4 1.6 0.4]\n",
      " [6.4 2.8 5.6 2.1]\n",
      " [7.9 3.8 6.4 2. ]\n",
      " [6.7 3.  5.2 2.3]\n",
      " [6.7 2.5 5.8 1.8]\n",
      " [6.8 3.2 5.9 2.3]\n",
      " [4.8 3.  1.4 0.3]\n",
      " [4.8 3.1 1.6 0.2]]\n",
      "模型预测结果：y_predict:[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 1 0 2 2 2 2 2 0 0]\n",
      "模型准确率 accuracy：0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "# 鸢尾花Bagging方法实现\n",
    "\n",
    "# 导入鸢尾花数据集\n",
    "from sklearn.datasets import load_iris\n",
    "# 导入随机森林分类器\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "# 导入训练集和测试集划分包\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 导入模型预测准确率性能\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 1、加载数据\n",
    "iris = load_iris()\n",
    "print(f'鸢尾花数据集描述：{iris.DESCR}')\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "print('参数：', X.size, y.size)\n",
    "\n",
    "\n",
    "# 2、数据集划分\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "print('训练集：', x_train.size, y_train.size)\n",
    "print('测试集：', x_test.size, y_test.size)\n",
    "\n",
    "# 3、构建Bagging模型\n",
    "rf_classifier = RandomForestClassifier(n_estimators = 5, random_state = 42)\n",
    "\n",
    "# 4、模型训练\n",
    "rf_classifier.fit(x_train, y_train)\n",
    "\n",
    "# 5、模型预测\n",
    "y_predict = rf_classifier.predict(x_test)\n",
    "print(f'模型预测原始数据：x_test:{x_test}')\n",
    "print(f'模型预测结果：y_predict:{y_predict}')\n",
    "\n",
    "# 6、模型评估\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "print(f'模型准确率 accuracy：{accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d478ff0de0aad082",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T12:02:04.387503Z",
     "start_time": "2024-08-31T12:02:04.124859Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集 特征值：[[4.6 3.6 1.  0.2]\n",
      " [5.7 4.4 1.5 0.4]], 目标值：[0 0]\n",
      "测试集 特征值：[[6.1 2.8 4.7 1.2]\n",
      " [5.7 3.8 1.7 0.3]], 目标值：[1 0]\n",
      "分类模型准确率： 1.0\n",
      "\n",
      "分类报告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 集成学习Boosting算法鸢尾花实现\n",
    "\n",
    "# 导入鸢尾花数据集\n",
    "from sklearn.datasets import load_iris\n",
    "# 导入训练集测试集划分模块\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 导入Boosting分类器（梯度增强分类器）\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "# 导入模型预测准确率和分类报告模块\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1、加载数据\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2、测试集训练集划分\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "print(f'训练集 特征值：{x_train[:2,]}, 目标值：{y_train[:2, ]}')\n",
    "print(f'测试集 特征值：{x_test[:2,]}, 目标值：{y_test[:2,]}')\n",
    "\n",
    "# 3、创建梯度提升树分类器\n",
    "gb_classifier = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# 4、模型训练\n",
    "gb_classifier.fit(x_train, y_train)\n",
    "\n",
    "# 5、使用模型进行预测\n",
    "y_pred = gb_classifier.predict(x_test)\n",
    "\n",
    "# 6、评估模型性能\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"分类模型准确率：\", accuracy)\n",
    "\n",
    "# 7、输出分类报告\n",
    "print(\"\\n分类报告：\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d41af65aec0e62b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-31T12:04:49.583168Z",
     "start_time": "2024-08-31T12:04:47.098042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集 特征值：[[4.6 3.6 1.  0.2]\n",
      " [5.7 4.4 1.5 0.4]], 目标值：[0 0]\n",
      "测试集 特征值：[[6.1 2.8 4.7 1.2]\n",
      " [5.7 3.8 1.7 0.3]], 目标值：[1 0]\n",
      "模型准确率： 1.0\n"
     ]
    }
   ],
   "source": [
    "# 集成学习Stacking算法鸢尾花实现\n",
    "\n",
    "# 导入鸢尾花数据集\n",
    "from sklearn.datasets import load_iris\n",
    "# 导入训练集测试集划分模块\n",
    "from sklearn.model_selection import train_test_split\n",
    "# 导入Boosting分类器（梯度增强分类器）\n",
    "from sklearn.ensemble import StackingClassifier, RandomForestClassifier, GradientBoostingClassifier\n",
    "# 导入模型预测准确率和分类报告模块\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 1、加载数据\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# 2、测试集训练集划分\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "print(f'训练集 特征值：{x_train[:2,]}, 目标值：{y_train[:2, ]}')\n",
    "print(f'测试集 特征值：{x_test[:2,]}, 目标值：{y_test[:2,]}')\n",
    "\n",
    "# 3、创建基本学习器（可以选择不同类型的学习器）\n",
    "base_learners = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100, random_state=42))\n",
    "]\n",
    "\n",
    "# 4、创建 Stacking 分类器\n",
    "stacking_classifier = StackingClassifier(estimators=base_learners,final_estimator=RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "\n",
    "# 5、训练模型\n",
    "stacking_classifier.fit(x_train, y_train)\n",
    "\n",
    "# 6、使用模型进行预测\n",
    "y_predict = stacking_classifier.predict(x_test)\n",
    "\n",
    "# 7、评估模型性能\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"模型准确率：\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca9f4969ddccbfd",
   "metadata": {},
   "source": [
    "# 总结：\n",
    "- 集成方法类型:\n",
    "    - 随机森林：随机森林是一种 Bagging 集成方法，它通过构建多个决策树，然后将它们的结果进行平均或投票来进行预测。\n",
    "    - 梯度提升树：梯度提升树是一种 Boosting 集成方法，它通过迭代训练决策树，每个新树试图纠正前一个树的错误。\n",
    "- 模型的构建方式:\n",
    "    - 随机森林：随机森林使用随机抽样技术（Bootstrap Aggregating）来创建多个训练数据子集，每个子集用于独立地训练一个决策树。此外，对于每个决策树的节点分裂，随机森林随机选择一个特征子集。这种随机性有助于减小模型的方差。\n",
    "    - 梯度提升树：梯度提升树是一个迭代的过程，每个新树的构建是在前一个树的残差上进行的。它尝试纠正前一个树的错误，因此每个新树都是基于前一个树的性能来构建的。\n",
    "- 多样性:\n",
    "    - 随机森林：多样性是通过随机采样和特征选择来实现的，因此随机森林的各个树之间通常具有较高的独立性。\n",
    "    - 梯度提升树：梯度提升树的多样性来自于每个新树纠正前一个树的错误，因此各个树之间通常存在强依赖性。\n",
    "- 并行化:\n",
    "    - 随机森林：由于树之间独立，随机森林容易并行化，可以有效地处理大规模数据。\n",
    "    - 梯度提升树：梯度提升树通常是串行训练的，因为每个新树的构建都依赖于前一个树的结果，所以不太容易并行化。\n",
    "- 超参数:\n",
    "    - 随机森林：主要的超参数包括树的数量（n_estimators）和随机选择特征的数量（max_features）等。\n",
    "    - 梯度提升树：主要的超参数包括树的数量（n_estimators）、学习率（learning_rate）、树的深度（max_depth）等。\n",
    "- 性能:\n",
    "    - 随机森林：随机森林通常在初始训练时表现较好，不容易过拟合，对噪声不敏感。\n",
    "    - 梯度提升树：梯度提升树通常需要仔细的调优，但它可以在合理的参数设置下达到非常高的性能。\n",
    "\n",
    "`综上所述:`随机森林适合用于高维数据和大规模数据集，通常不需要太多的调优。梯度提升树通常在性能上更强大，但需要更多的调参和计算资源。选择哪种方法通常取决于具体问题和数据的特点。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
